{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from utils.neural_network import train_nn, get_device\n",
    "\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing_v2 import Preprocessing, get_gt_diff_logs\n",
    "from utils.results import bootstrap_ensemble, interactive_plot_predictions, summarize_results\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from postprocessing.arima import smooth_nn_predictions_with_arima_auto, smooth_nn_predictions_with_arima\n",
    "\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9\n",
    "MODE = \"diff\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()\n",
    "data['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(epsilon=EPS, gdp_diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, x_high_freq = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, \n",
    "                                                                  mode=MODE,\n",
    "                                                                  take_log_diff_gdp=True,\n",
    "                                                                  gt_trend_removal=False, \n",
    "                                                                  keep_pca_components=180, \n",
    "                                                                  noisy_data_stds=[0.001, 0.005, 0.01], \n",
    "                                                                  add_encoded_month=False, \n",
    "                                                                  gt_data_transformations=[get_gt_diff_logs], other_params={'plot_pca': False})\n",
    "\n",
    "print(all_gts['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss, validation_loss, validation_r_squared, mse_losses = train_nn(X_train, \n",
    "                                                                       y_train, \n",
    "                                                                       X_valid, \n",
    "                                                                       y_valid, \n",
    "                                                                       num_epochs=100, \n",
    "                                                                       learning_rate=1e-4, \n",
    "                                                                       weight_decay=1e-2, \n",
    "                                                                       verbose=True,\n",
    "                                                                       seed=SEED)\n",
    "\n",
    "# Compute best r squared with epoch\n",
    "best_r_squared = max(validation_r_squared)\n",
    "best_epoch = validation_r_squared.index(best_r_squared)\n",
    "\n",
    "print(f\"Best r squared: {best_r_squared} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High frequency predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Predict for the high frequency data\n",
    "device = get_device(False)\n",
    "y_pred_high_freq = model(torch.tensor(x_high_freq, dtype=torch.float32).to(device)).cpu().detach().numpy().flatten()\n",
    "y_pred_high_freq_country = pd.DataFrame({'date': preprocessor.dates_high_freq, 'country': preprocessor.country_high_freq, 'y_pred': y_pred_high_freq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true GDPs\n",
    "country_train = preprocessor.country_train\n",
    "country_valid = preprocessor.country_valid\n",
    "all_true_gdps_country = np.concatenate([y_train, y_valid], axis=0)\n",
    "all_dates = pd.concat([preprocessor.dates_train, preprocessor.dates_valid], axis=0)\n",
    "all_true_gdps_country = pd.DataFrame({'date': all_dates, 'country': np.concatenate([country_train, country_valid], axis=0), 'y_true': np.concatenate([y_train, y_valid], axis=0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the predicions to be plotted\n",
    "predictions = y_pred_high_freq_country\n",
    "\n",
    "# Add the true gdp for the points that we have\n",
    "predictions = predictions.merge(all_true_gdps_country, on=[\"date\", \"country\"], how=\"left\")\n",
    "\n",
    "# Rename y_pred to y_pred_high_freq\n",
    "predictions = predictions.rename(columns={\"y_pred\": \"y_pred_high_freq\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot data for the selected country\n",
    "def plot_by_country(selected_country):\n",
    "    filtered_data = predictions[predictions[\"country\"] == selected_country]\n",
    "    cutoff_date = predictions['date'].quantile(TRAIN_PROPORTION)\n",
    "    non_nans = filtered_data[\"y_true\"].notna()\n",
    "    y_trues = filtered_data[\"y_true\"][non_nans]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(filtered_data[\"date\"], filtered_data[\"y_pred_high_freq\"], label=\"Predicted\")\n",
    "    plt.plot(filtered_data[\"date\"][non_nans], y_trues, label=\"True\")\n",
    "    #plt.fill_between(filtered_data[\"date\"], filtered_data[\"y_pred_high_freq\"] - 3 * filtered_data[\"stdv\"], filtered_data[\"y_pred_high_freq\"] + 3 * filtered_data[\"stdv\"], alpha=0.2, color=\"blue\")\n",
    "    plt.title(f\"High frequency predictions vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.axvline(x=cutoff_date, color='red', linestyle='--', label=f'Validation Start ({TRAIN_PROPORTION}%)')\n",
    "    plt.legend(title=\"Legend\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "countries.sort()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the plot function\n",
    "interact(plot_by_country, selected_country=dropdown)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ARIMA smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2\n",
    "d = 1\n",
    "q = 1\n",
    "adjusted_predictions = smooth_nn_predictions_with_arima(predictions, p, d, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_country(selected_country):\n",
    "    adjusted_filtered_data = adjusted_predictions[adjusted_predictions[\"country\"] == selected_country]\n",
    "    filtered_data = predictions[predictions[\"country\"] == selected_country]\n",
    "    cutoff_date = predictions['date'].quantile(TRAIN_PROPORTION)\n",
    "    non_nans = filtered_data[\"y_true\"].notna()\n",
    "    y_trues = filtered_data[\"y_true\"][non_nans]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(filtered_data[\"date\"], filtered_data[\"y_pred_high_freq\"], label=\"Predicted\", alpha=0.5)\n",
    "    plt.plot(adjusted_filtered_data[\"date\"], adjusted_filtered_data[\"y_pred_high_freq\"], label=\"Adjusted Predicted\")\n",
    "    plt.plot(filtered_data[\"date\"][non_nans], y_trues, label=\"True\")\n",
    "    plt.title(f\"High frequency predictions vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.axvline(x=cutoff_date, color='red', linestyle='--', label=f'Validation Start ({TRAIN_PROPORTION}%)')\n",
    "    plt.legend(title=\"Legend\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the adjusted predictions\n",
    "interact(plot_by_country, selected_country=dropdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
