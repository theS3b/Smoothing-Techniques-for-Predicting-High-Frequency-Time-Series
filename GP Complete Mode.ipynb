{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing_v2 import Preprocessing, get_gt_diff_logs\n",
    "from utils.results import compute_rsquared\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9\n",
    "MODE = \"pct\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(epsilon=EPS, gdp_diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, x_high_freq = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, \n",
    "                                                                  mode=\"diff\",\n",
    "                                                                  take_log_diff_gdp=True, \n",
    "                                                                  gt_trend_removal=False, \n",
    "                                                                  keep_pca_components=180, \n",
    "                                                                  noisy_data_stds=[0.001, 0.005, 0.01], \n",
    "                                                                  add_encoded_month=False, \n",
    "                                                                  gt_data_transformations=[get_gt_diff_logs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gts['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "# Simplified kernel (removed WhiteKernel to reduce complexity)\n",
    "kernel = C(1.0, (1e-1, 10)) * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10))\n",
    "\n",
    "# Create the GaussianProcessRegressor with minimal restarts\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1)\n",
    "\n",
    "# Define an even smaller parameter grid\n",
    "param_dist = {\n",
    "    \"kernel__k1__constant_value\": [1, 10],  # Reduced range\n",
    "    \"kernel__k2__length_scale\": [1, 10],  # Reduced range\n",
    "    \"alpha\": [1e-1],  # Single value\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with very few samples and minimal folds\n",
    "random_search = RandomizedSearchCV(\n",
    "    gp,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=3,  # Only 3 parameter settings sampled\n",
    "    cv=2,  # Minimal cross-validation\n",
    "    n_jobs=-1,  # Parallel processing\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.set_params(**random_search.best_params_)\n",
    "gp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred, sigma = gp.predict(X_valid, return_std=True)\n",
    "y_pred_train, sigma_train = gp.predict(X_train, return_std=True)\n",
    "y_pred_high_freq, sigma_high_freq = gp.predict(x_high_freq, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_valid = mean_squared_error(y_valid, y_pred)\n",
    "print(f\"Training MSE: {mse_train:.4f}\")\n",
    "print(f\"Validation MSE: {mse_valid:.4f}\")\n",
    "\n",
    "# MAPE\n",
    "mape_train = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100\n",
    "mape_valid = np.mean(np.abs((y_valid - y_pred) / y_valid)) * 100\n",
    "print(f\"Training MAPE: {mape_train:.2f}%\")\n",
    "print(f\"Validation MAPE: {mape_valid:.2f}%\")\n",
    "\n",
    "# R^2\n",
    "r2_train = compute_rsquared(y_train, y_pred_train)\n",
    "r2_valid = compute_rsquared(y_valid, y_pred)\n",
    "print(f\"Training R^2: {r2_train:.4f}\")\n",
    "print(f\"Validation R^2: {r2_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the result by country\n",
    "y_pred_country = pd.DataFrame({'date': preprocessor.dates_valid, 'country': preprocessor.country_valid, 'y_pred': y_pred, 'y_true': y_valid, 'y_std': sigma})\n",
    "y_pred_train_country = pd.DataFrame({'date': preprocessor.dates_train, 'country': preprocessor.country_train, 'y_pred': y_pred_train, 'y_true': y_train, 'y_std': sigma_train})\n",
    "y_pred_high_freq_country = pd.DataFrame({'date': preprocessor.dates_high_freq, 'country': preprocessor.country_high_freq, 'y_pred': y_pred_high_freq, 'y_std': sigma_high_freq})\n",
    "\n",
    "# Put together the train and the validation set\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n",
    "predictions_high_freq = pd.concat([y_pred_train_country, y_pred_high_freq_country])\n",
    "\n",
    "# Melting the dataframe for better plotting\n",
    "predictions_melted = predictions.melt(\n",
    "    id_vars=[\"date\", \"country\"], value_vars=[\"y_pred\", \"y_true\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "predictions_high_freq_melted = predictions_high_freq.melt(\n",
    "    id_vars=[\"date\", \"country\"], value_vars=[\"y_pred\", \"y_true\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions per country, per date\n",
    "def plot_by_country(selected_country, country_valid):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(y_pred_country[country_valid == selected_country]['y_true'].values, label=\"True\")\n",
    "    plt.plot(y_pred_country[country_valid == selected_country]['y_pred'].values, label=\"Predicted\")\n",
    "    plt.fill_between(np.arange(len(y_valid[country_valid == selected_country])), y_pred_country[country_valid == selected_country]['y_pred'].values - 3 * y_pred_country[country_valid == selected_country]['y_std'].values, y_pred_country[country_valid == selected_country]['y_pred'].values + 3 * y_pred_country[country_valid == selected_country]['y_std'].values, alpha=0.05, color=\"red\")\n",
    "    plt.fill_between(np.arange(len(y_valid[country_valid == selected_country])), y_pred_country[country_valid == selected_country]['y_pred'].values - 1.96 * y_pred_country[country_valid == selected_country]['y_std'].values, y_pred_country[country_valid == selected_country]['y_pred'].values + 1.96 * y_pred_country[country_valid == selected_country]['y_std'].values, alpha=0.2)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"GDP\")\n",
    "    plt.title(f\"{selected_country}\")\n",
    "    plt.legend()\n",
    "\n",
    "interact(plot_by_country, selected_country=np.unique(preprocessor.country_valid), country_valid=widgets.fixed(preprocessor.country_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot data with confidence intervals for the selected country\n",
    "def plot_by_country_with_confidence(selected_country):\n",
    "    filtered_data = predictions_melted[predictions_melted[\"country\"] == selected_country]\n",
    "    cutoff_date = preprocessor.splitting_date # predictions['date'].quantile(TRAIN_PROPORTION)\n",
    "\n",
    "\n",
    "    unmelted_data = predictions[(predictions[\"country\"] == selected_country)]\n",
    "    unmelted_data = unmelted_data.sort_values(\"date\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot predictions and true values\n",
    "    sns.lineplot(\n",
    "        data=filtered_data,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", style=\"Type\", markers=True, dashes=False\n",
    "    )\n",
    "    \n",
    "    plt.fill_between(\n",
    "        unmelted_data[\"date\"],\n",
    "        unmelted_data[\"y_pred\"] - 1.96 * unmelted_data[\"y_std\"],\n",
    "        unmelted_data[\"y_pred\"] + 1.96 * unmelted_data[\"y_std\"],\n",
    "        color=\"red\", alpha=0.2, label=\"Confidence Interval\"\n",
    "    )\n",
    "    \n",
    "    # Add a vertical line to indicate where validation starts\n",
    "    plt.axvline(x=cutoff_date, color='red', linestyle='--', label=f'Validation Start ({TRAIN_PROPORTION}%)')\n",
    "    \n",
    "    # Enhancing the plot\n",
    "    plt.title(f\"Prediction vs True Values with Confidence Intervals for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend(title=\"Legend\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the updated plot function\n",
    "interact(plot_by_country_with_confidence, selected_country=dropdown)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot data with confidence intervals for the selected country\n",
    "def plot_by_country_with_confidence(selected_country):\n",
    "    filtered_data = predictions_high_freq_melted[predictions_high_freq_melted[\"country\"] == selected_country]\n",
    "    cutoff_date = preprocessor.splitting_date # predictions['date'].quantile(TRAIN_PROPORTION)\n",
    "\n",
    "    unmelted_data = predictions_high_freq[(predictions_high_freq[\"country\"] == selected_country)]\n",
    "    unmelted_data = unmelted_data.sort_values(\"date\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot predictions and true values\n",
    "    sns.lineplot(\n",
    "        data=filtered_data,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", style=\"Type\", markers=True, dashes=False\n",
    "    )\n",
    "    \n",
    "    plt.fill_between(\n",
    "        unmelted_data[\"date\"],\n",
    "        unmelted_data[\"y_pred\"] - 1.96 * unmelted_data[\"y_std\"],\n",
    "        unmelted_data[\"y_pred\"] + 1.96 * unmelted_data[\"y_std\"],\n",
    "        color=\"red\", alpha=0.2, label=\"Confidence Interval\"\n",
    "    )\n",
    "    \n",
    "    # Add a vertical line to indicate where validation starts\n",
    "    plt.axvline(x=cutoff_date, color='red', linestyle='--', label=f'Validation Start ({TRAIN_PROPORTION}%)')\n",
    "    \n",
    "    # Enhancing the plot\n",
    "    plt.title(f\"Prediction vs True Values with Confidence Intervals for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend(title=\"Legend\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions_high_freq_melted[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the updated plot function\n",
    "interact(plot_by_country_with_confidence, selected_country=dropdown)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmelted_data = predictions_high_freq_melted[(predictions_high_freq_melted[\"country\"] == 'Canada')]\n",
    "unmelted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
