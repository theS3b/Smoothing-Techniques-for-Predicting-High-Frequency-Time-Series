{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from utils.neural_network import train_nn, get_device, batch_data_by_country\n",
    "\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing_v2 import Preprocessing, get_gt_diff_logs\n",
    "import utils.results as results\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9\n",
    "MODE = \"pct\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()\n",
    "data['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(epsilon=EPS, gdp_diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, x_hf = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, \n",
    "                                                                  mode=\"diff\", \n",
    "                                                                  take_log_diff_gdp=True,\n",
    "                                                                  gt_trend_removal=False, \n",
    "                                                                  keep_pca_components=180, \n",
    "                                                                  noisy_data_stds=[0.005, 0.05, 0.1], \n",
    "                                                                  add_encoded_month=False, \n",
    "                                                                  gt_data_transformations=[get_gt_diff_logs], other_params={'plot_pca': False})\n",
    "\n",
    "x_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "x_valid_t = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "y_valid_t = torch.tensor(y_valid, dtype=torch.float32).to(device)\n",
    "x_hf_t = torch.tensor(x_hf, dtype=torch.float32).to(device)\n",
    "\n",
    "print(all_gts['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.dates_high_freq\n",
    "preprocessor.country_high_freq\n",
    "\n",
    "df_hf = pd.DataFrame({\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'data': [x_hf[i] for i in range(len(x_hf))]\n",
    "})\n",
    "\n",
    "df_x_train = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train.copy(),\n",
    "    'country': preprocessor.country_train.copy(),\n",
    "    'data': [X_train[i].copy() for i in range(len(X_train))],\n",
    "    'y_true': [y_train[i].copy() for i in range(len(y_train))]\n",
    "})\n",
    "\n",
    "df_hf['date'] = pd.to_datetime(df_hf['date'])\n",
    "df_x_train['date'] = pd.to_datetime(df_x_train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "def smoothness_loss(y_pred, smoothness_weight_ord1=0.33, smoothness_weight_ord2=0.33):\n",
    "    if y_pred.shape[1] > 1:\n",
    "        # Compute the loss related to smoothness\n",
    "        smoothness_loss_ord1 = 0\n",
    "        for i in range(1, 4):\n",
    "            # Divide by 3 because dt is 1/3 month, and by 4 because we have 4 time points and we average them\n",
    "            smoothness_loss_ord1 += torch.linalg.norm((y_pred[:, i, :] - y_pred[:, i - 1, :])) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        # Constraint on the double derivative\n",
    "        smoothness_loss_ord2 = 0\n",
    "        for i in range(2, 4):\n",
    "            smoothness_loss_ord2 += torch.linalg.norm(y_pred[:, i, :] - 2 * y_pred[:, i - 1, :] + y_pred[:, i - 2, :]) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        return smoothness_loss_ord1 * smoothness_weight_ord1 + smoothness_loss_ord2 * smoothness_weight_ord2\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")\n",
    "\n",
    "def custom_loss(y_pred, y_true, smoothness_weight_ord1=0.33, smoothness_weight_ord2=0.33):\n",
    "    # Compute usual MSE on actual GDP values\n",
    "    mse = MSELoss()\n",
    "\n",
    "    if y_pred.shape[1] > 1:\n",
    "        prediction_loss = mse(y_pred[:, 4, :], y_true)  # First column is the actual GDP\n",
    "\n",
    "        smooth_loss = smoothness_loss(y_pred, smoothness_weight_ord1, smoothness_weight_ord2)\n",
    "\n",
    "        return prediction_loss + smooth_loss\n",
    "    elif y_pred.shape[1] == 1:\n",
    "        return mse(y_pred, y_true)\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data, filtered_countries, filtered_dates, y_augmented_train = batch_data_by_country(df_x_train, \n",
    "                                                                                              df_hf, \n",
    "                                                                                              4, \n",
    "                                                                                              0, \n",
    "                                                                                              ['y_true'], \n",
    "                                                                                              False)\n",
    "\n",
    "y_augmented_train = np.array(y_augmented_train).squeeze().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "validation_r_squareds = []\n",
    "ord1_smoothness_weights = np.logspace(-1, 2, 10)\n",
    "ord2_smoothness_weights = np.logspace(2, 3.2, 10)\n",
    "smoothness_losses = []\n",
    "global_mse_losses = []\n",
    "parameters = []\n",
    "\n",
    "for ord1_weight in tqdm(ord1_smoothness_weights):\n",
    "    for ord2_weight in tqdm(ord2_smoothness_weights, leave=False):\n",
    "        model, training_loss, validation_loss, validation_r_squared, mse_loss = train_nn(augmented_data, \n",
    "                                                                            y_augmented_train, \n",
    "                                                                            X_valid, \n",
    "                                                                            y_valid, \n",
    "                                                                            num_epochs=100, \n",
    "                                                                            learning_rate=5e-4, \n",
    "                                                                            weight_decay=5e-2, \n",
    "                                                                            verbose=False,\n",
    "                                                                            custom_loss=lambda x, y: custom_loss(x, y, ord1_weight, smoothness_weight_ord2=ord2_weight),\n",
    "                                                                            current_gdp_idx=4,\n",
    "                                                                            seed=SEED)\n",
    "        parameters.append((ord1_weight, ord2_weight))\n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_r_squareds.append(validation_r_squared)\n",
    "        global_mse_losses.append(mse_loss)\n",
    "\n",
    "        y_pred_hf = model(torch.tensor(x_hf, dtype=torch.float32).to(device)).clone().detach().cpu().numpy().squeeze()\n",
    "\n",
    "        df_preds = pd.DataFrame({\n",
    "            'date': preprocessor.dates_high_freq,\n",
    "            'country': preprocessor.country_high_freq,\n",
    "            'data': [y_pred_hf[i] for i in range(len(x_hf))]\n",
    "        })\n",
    "\n",
    "        smoothness_values = []\n",
    "        for country in df_preds['country'].unique():\n",
    "            country_data = df_preds[df_preds['country'] == country]\n",
    "            \n",
    "            smoothness_values_per_func = []\n",
    "            for smoothness_func in results.all_smoothness_metrics:\n",
    "                smoothness_values_per_func.append(smoothness_func(country_data['data'].values))\n",
    "\n",
    "            smoothness_values.append(smoothness_values_per_func)\n",
    "\n",
    "        smoothness_values = np.array(smoothness_values)\n",
    "\n",
    "        # Store different means of the smoothness values\n",
    "        smoothness_losses.append([np.mean(smoothness_values), np.expm1(np.mean(np.log1p(smoothness_values))), len(smoothness_values) / np.sum(1 / smoothness_values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_top_filtered_percent(data, x_vals, y_vals, title, xlabel, ylabel, cmap='viridis', threshold=5):\n",
    "    # Compute the 90th percentile threshold\n",
    "    valid_data = data[np.isfinite(data)]\n",
    "    if len(valid_data) == 0:\n",
    "        print(\"No valid data to plot.\")\n",
    "        return\n",
    "    #threshold = np.percentile(valid_data, 90)\n",
    "    \n",
    "    # Mask values below the threshold\n",
    "    data_filtered = np.where((data >= -threshold) & (data <= threshold), data, np.nan)\n",
    "    \n",
    "    im = plt.imshow(data_filtered, origin='lower', \n",
    "                    extent=[min(x_vals), max(x_vals), min(y_vals), max(y_vals)], \n",
    "                    aspect='auto', cmap=cmap) #, norm=norm)\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label('Metric Value')\n",
    "    plt.title(title + f' (Between -{threshold} and {threshold})')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = len(ord1_smoothness_weights)\n",
    "N2 = len(ord2_smoothness_weights)\n",
    "\n",
    "# Create mapping from (ord1_weight, ord2_weight) to grid indices\n",
    "ord1_to_idx = {w: i for i, w in enumerate(ord1_smoothness_weights)}\n",
    "ord2_to_idx = {w: i for i, w in enumerate(ord2_smoothness_weights)}\n",
    "\n",
    "# Initialize empty arrays for final metrics\n",
    "final_train_loss = np.zeros((N1, N2))\n",
    "final_val_loss = np.zeros((N1, N2))\n",
    "final_val_r2 = np.zeros((N1, N2))\n",
    "final_smoothness_mean = np.zeros((N1, N2))\n",
    "final_validation_mse_loss = np.zeros((N1, N2))  # example if needed\n",
    "final_training_mse_loss = np.zeros((N1, N2))  # example if needed\n",
    "\n",
    "for (ord1, ord2), tr_loss, val_loss, val_r2, mse_loss, sm_loss in zip(parameters, \n",
    "                                                                    training_losses, \n",
    "                                                                    validation_losses, \n",
    "                                                                    validation_r_squareds, \n",
    "                                                                    global_mse_losses, \n",
    "                                                                    smoothness_losses):\n",
    "    i = ord1_to_idx[ord1]\n",
    "    j = ord2_to_idx[ord2]\n",
    "\n",
    "    final_train_loss[i, j] = tr_loss[-1] if len(tr_loss) > 0 else np.nan\n",
    "    final_val_loss[i, j] = val_loss[-1] if len(val_loss) > 0 else np.nan\n",
    "    final_val_r2[i, j] = val_r2[-1] if len(val_r2) > 0 else np.nan\n",
    "    \n",
    "    final_smoothness_mean[i, j] = sm_loss[1] if len(sm_loss) > 0 else np.nan\n",
    "    final_validation_mse_loss[i, j] = mse_loss[-1][1] if len(mse_loss) > 0 else np.nan\n",
    "    final_training_mse_loss[i, j] = mse_loss[-1][0] if len(mse_loss) > 0 else np.nan\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot final training loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_heatmap_top_filtered_percent(-final_train_loss, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='Final Training Loss', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "# Example usage with final_validation_loss (or any other metric you computed):\n",
    "plot_heatmap_top_filtered_percent(-final_val_loss, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "                            title='Final Validation Loss', \n",
    "                            xlabel='ord1_weight', \n",
    "                            ylabel='ord2_weight')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_heatmap_top_filtered_percent(final_val_r2, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='Final Validation - R^2', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "\n",
    "# Plot final smoothness mean metric\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_heatmap_top_filtered_percent(-final_smoothness_mean, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='Final Smoothness (Mean)', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_heatmap_top_filtered_percent(-final_training_mse_loss, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='Final Training MSE Loss', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_heatmap_top_filtered_percent(-final_validation_mse_loss, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='Final Validation MSE Loss', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothness_weight_ord1=40\n",
    "smoothness_weight_ord2=40\n",
    "model, training_loss, validation_loss, validation_r_squared, global_mse_losses = train_nn(augmented_data, \n",
    "                                                                        y_augmented_train, \n",
    "                                                                        X_valid, \n",
    "                                                                        y_valid, \n",
    "                                                                        num_epochs=100, \n",
    "                                                                        learning_rate=5e-4, \n",
    "                                                                        weight_decay=5e-2, \n",
    "                                                                        verbose=True,\n",
    "                                                                        current_gdp_idx=4,\n",
    "                                                                        custom_loss=lambda x, y: custom_loss(x, y, smoothness_weight_ord1, smoothness_weight_ord2),\n",
    "                                                                        seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(False)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = model(x_valid_t).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_train = model(x_train_t).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_hf = model(x_hf_t).clone().detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Associate the result by country and add 'Set' column\n",
    "y_pred_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_valid,\n",
    "    'country': preprocessor.country_valid,\n",
    "    'y_pred': y_pred,\n",
    "    'y_true': y_valid,\n",
    "    'Set': 'Validation'\n",
    "})\n",
    "\n",
    "y_pred_train_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train,\n",
    "    'country': preprocessor.country_train,\n",
    "    'y_pred': y_pred_train,\n",
    "    'y_true': y_train,\n",
    "    'Set': 'Training'\n",
    "})\n",
    "\n",
    "# Put together the train and the validation set\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n",
    "predictions_hf = pd.DataFrame({\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'y_pred': y_pred_hf,\n",
    "    'Set': 'High Frequency'\n",
    "})\n",
    "\n",
    "\n",
    "# Melting the dataframe for better plotting\n",
    "predictions_melted = predictions.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\", \"y_true\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "predictions_hf_melted = predictions_hf.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Function to plot data for the selected country\n",
    "def plot_by_country(selected_country):\n",
    "    filtered_data = predictions_melted[predictions_melted[\"country\"] == selected_country]\n",
    "    filtered_data_hf = predictions_hf_melted[predictions_hf_melted[\"country\"] == selected_country]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=filtered_data_hf,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True, palette=\"Set3\"\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        data=filtered_data,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Prediction vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend(title=\"Legend\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the plot function\n",
    "interact(plot_by_country, selected_country=dropdown)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per country\n",
    "smoothness_values = []\n",
    "for country in df_preds['country'].unique():\n",
    "    country_data = df_preds[df_preds['country'] == country]\n",
    "    \n",
    "    smoothness_values_per_func = []\n",
    "    for smoothness_func in results.all_smoothness_metrics:\n",
    "        smoothness_values_per_func.append(smoothness_func(country_data['data'].values))\n",
    "\n",
    "    smoothness_values.append(smoothness_values_per_func)\n",
    "\n",
    "smoothness_values = np.array(smoothness_values)\n",
    "\n",
    "plt.barh(range(smoothness_values.shape[1]), np.log(smoothness_values.mean(axis=0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(smoothness_values), np.exp(np.mean(np.log(smoothness_values))), len(smoothness_values) / np.sum(1 / smoothness_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Fourrier Transform Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_analysis(preds, cutoff=0.05): \n",
    "    fft = np.fft.fft(preds)\n",
    "    freqs = np.fft.fftfreq(len(preds))\n",
    "\n",
    "    filtered = np.copy(fft)\n",
    "    filtered[np.abs(freqs) > cutoff] = 0\n",
    "\n",
    "    y_filtered = np.fft.ifft(filtered)\n",
    "    plt.plot(preds, label='Original')\n",
    "    plt.plot(y_filtered, label='Filtered')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Per country\n",
    "for country in df_preds['country'].unique():\n",
    "    country_data = df_preds[df_preds['country'] == country]\n",
    "    fft_analysis(country_data['data'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
