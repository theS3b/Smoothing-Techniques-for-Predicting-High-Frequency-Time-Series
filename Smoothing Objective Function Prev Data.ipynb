{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from utils.neural_network import train_nn, get_device, batch_data_by_country\n",
    "\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing_v2 import Preprocessing, get_gt_diff_logs\n",
    "import utils.results as results\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9\n",
    "MODE = \"pct\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()\n",
    "data['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(epsilon=EPS, gdp_diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, x_hf = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, \n",
    "                                                                  mode=\"diff\", \n",
    "                                                                  take_log_diff_gdp=True,\n",
    "                                                                  gt_trend_removal=False, \n",
    "                                                                  keep_pca_components=180, \n",
    "                                                                  noisy_data_stds=[0.005, 0.05, 0.1], \n",
    "                                                                  add_encoded_month=False, \n",
    "                                                                  gt_data_transformations=[get_gt_diff_logs], other_params={'plot_pca': False})\n",
    "\n",
    "x_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "x_valid_t = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "y_valid_t = torch.tensor(y_valid, dtype=torch.float32).to(device)\n",
    "x_hf_t = torch.tensor(x_hf, dtype=torch.float32).to(device)\n",
    "\n",
    "print(all_gts['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.dates_high_freq\n",
    "preprocessor.country_high_freq\n",
    "\n",
    "df_hf = pd.DataFrame({\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'data': [x_hf[i] for i in range(len(x_hf))]\n",
    "})\n",
    "\n",
    "df_x_train = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train.copy(),\n",
    "    'country': preprocessor.country_train.copy(),\n",
    "    'data': [X_train[i].copy() for i in range(len(X_train))],\n",
    "    'y_true': [y_train[i].copy() for i in range(len(y_train))]\n",
    "})\n",
    "\n",
    "df_hf['date'] = pd.to_datetime(df_hf['date'])\n",
    "df_x_train['date'] = pd.to_datetime(df_x_train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "def smoothness_loss(y_pred, smoothness_weight_ord1=0.33, smoothness_weight_ord2=0.33):\n",
    "    if y_pred.shape[1] > 1:\n",
    "        # Compute the loss related to smoothness\n",
    "        smoothness_loss_ord1 = 0\n",
    "        for i in range(1, 4):\n",
    "            # Divide by 3 because dt is 1/3 month, and by 4 because we have 4 time points and we average them\n",
    "            smoothness_loss_ord1 += torch.linalg.norm((y_pred[:, i, :] - y_pred[:, i - 1, :])) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        # Constraint on the double derivative\n",
    "        smoothness_loss_ord2 = 0\n",
    "        for i in range(2, 4):\n",
    "            smoothness_loss_ord2 += torch.linalg.norm(y_pred[:, i, :] - 2 * y_pred[:, i - 1, :] + y_pred[:, i - 2, :]) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        return smoothness_loss_ord1 * smoothness_weight_ord1 + smoothness_loss_ord2 * smoothness_weight_ord2\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")\n",
    "\n",
    "def custom_loss(y_pred, y_true, smoothness_weight_ord1=0.33, smoothness_weight_ord2=0.33):\n",
    "    # Compute usual MSE on actual GDP values\n",
    "    mse = MSELoss()\n",
    "\n",
    "    if y_pred.shape[1] > 1:\n",
    "        prediction_loss = mse(y_pred[:, -1, :], y_true)  # First column is the actual GDP\n",
    "\n",
    "        smooth_loss = smoothness_loss(y_pred, smoothness_weight_ord1, smoothness_weight_ord2)\n",
    "\n",
    "        return prediction_loss + smooth_loss\n",
    "    elif y_pred.shape[1] == 1:\n",
    "        return mse(y_pred, y_true)\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data, filtered_countries, filtered_dates, y_augmented_train, _ = batch_data_by_country(df_x_train, \n",
    "                                                                                              df_hf, \n",
    "                                                                                              3, \n",
    "                                                                                              0, \n",
    "                                                                                              ['y_true'], \n",
    "                                                                                              False)\n",
    "\n",
    "y_augmented_train = np.array(y_augmented_train).squeeze().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "ord1_smoothness_weights = np.logspace(-1, 2, 30)\n",
    "ord2_smoothness_weights = np.logspace(1, 3, 30)\n",
    "\n",
    "def train_and_evaluate(ord1_weight, ord2_weight):\n",
    "    model, training_loss, validation_loss, validation_r_squared, mse_loss = train_nn(\n",
    "        augmented_data,\n",
    "        y_augmented_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        num_epochs=100,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-2,\n",
    "        verbose=False,\n",
    "        custom_loss=lambda x, y: custom_loss(x, y, ord1_weight, smoothness_weight_ord2=ord2_weight),\n",
    "        current_gdp_idx=-1,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    y_pred_hf = model(torch.tensor(x_hf, dtype=torch.float32).to(device)).clone().detach().cpu().numpy().squeeze()\n",
    "    smoothness_loss = results.measure_smoothness(y_pred_hf, preprocessor.dates_high_freq, preprocessor.country_high_freq)\n",
    "\n",
    "    return (ord1_weight, ord2_weight, training_loss, validation_loss, validation_r_squared, mse_loss, smoothness_loss)\n",
    "\n",
    "# Run in parallel\n",
    "param_combinations = [(ord1, ord2) for ord1 in ord1_smoothness_weights for ord2 in ord2_smoothness_weights]\n",
    "\n",
    "results = Parallel(n_jobs=-1, backend='loky')(\n",
    "    delayed(train_and_evaluate)(ord1, ord2) for ord1, ord2 in tqdm(param_combinations)\n",
    ")\n",
    "\n",
    "# Unpack results\n",
    "ord1_weight, ord2_weight, training_loss, validation_loss, validation_r_squared, mse_loss, smoothness_loss = zip(*results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord1_weight, ord2_weight, training_loss, validation_loss, validation_r_squared, mse_loss, smoothness_loss = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_top_filtered_percent(data, x_vals, y_vals, title, xlabel, ylabel, cmap='viridis', threshold=5):\n",
    "    # Compute the 90th percentile threshold\n",
    "    valid_data = data[np.isfinite(data)]\n",
    "    if len(valid_data) == 0:\n",
    "        print(\"No valid data to plot.\")\n",
    "        return\n",
    "    #threshold = np.percentile(valid_data, 90)\n",
    "    \n",
    "    # Mask values below the threshold\n",
    "    data_filtered = np.where((data >= -threshold) & (data <= threshold), data, np.nan)\n",
    "    \n",
    "    im = plt.imshow(data_filtered, origin='lower', \n",
    "                    extent=[min(x_vals), max(x_vals), min(y_vals), max(y_vals)], \n",
    "                    aspect='auto', cmap=cmap) #, norm=norm)\n",
    "    \n",
    "    # Hilight the maximum value\n",
    "    max_val = np.nanmax(data_filtered)\n",
    "    max_idx = np.where(data_filtered == max_val)\n",
    "    # Centered in the middle of the square\n",
    "    plt.scatter(x_vals[max_idx[1]],\n",
    "                y_vals[max_idx[0]],\n",
    "                color='red', marker='x', s=30)\n",
    "\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label('Metric Value')\n",
    "    plt.title(title + f' (Between -{threshold} and {threshold})')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = len(ord1_smoothness_weights)\n",
    "N2 = len(ord2_smoothness_weights)\n",
    "\n",
    "# Create mapping from (ord1_weight, ord2_weight) to grid indices\n",
    "ord1_to_idx = {w: i for i, w in enumerate(ord1_smoothness_weights)}\n",
    "ord2_to_idx = {w: i for i, w in enumerate(ord2_smoothness_weights)}\n",
    "\n",
    "# Initialize empty arrays for final metrics\n",
    "final_train_loss = np.zeros((N1, N2))\n",
    "final_val_loss = np.zeros((N1, N2))\n",
    "final_val_r2 = np.zeros((N1, N2))\n",
    "final_smoothness_mean = np.zeros((N1, N2))\n",
    "final_validation_mse_loss = np.zeros((N1, N2))  # example if needed\n",
    "final_training_mse_loss = np.zeros((N1, N2))  # example if needed\n",
    "\n",
    "for ord1, ord2, tr_loss, val_loss, val_r2, mse_loss, sm_loss in zip(ord1_weight, ord2_weight, training_loss, validation_loss, validation_r_squared, mse_loss, smoothness_loss):\n",
    "    i = ord1_to_idx[ord1]\n",
    "    j = ord2_to_idx[ord2]\n",
    "\n",
    "    final_train_loss[i, j] = tr_loss[-1] if len(tr_loss) > 0 else np.nan\n",
    "    final_val_loss[i, j] = val_loss[-1] if len(val_loss) > 0 else np.nan\n",
    "    final_val_r2[i, j] = val_r2[-1] if len(val_r2) > 0 else np.nan\n",
    "    \n",
    "    final_smoothness_mean[i, j] = sm_loss[1] if len(sm_loss) > 0 else np.nan\n",
    "    # final_validation_mse_loss[i, j] = mse_loss[-1][1] if len(mse_loss) > 0 else np.nan\n",
    "    # final_training_mse_loss[i, j] = mse_loss[-1][0] if len(mse_loss) > 0 else np.nan\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_heatmap_top_filtered_percent(final_val_r2, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='Final Validation R^2', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "\n",
    "# Plot final smoothness mean metric\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_heatmap_top_filtered_percent(-final_smoothness_mean, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='Final Smoothness Strength', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "normed_final_val_r2 = final_val_r2 - np.nanmin(final_val_r2) / (np.nanmax(final_val_r2) - np.nanmin(final_val_r2))\n",
    "normed_final_smoothness_mean = final_smoothness_mean - np.nanmin(final_smoothness_mean) / (np.nanmax(final_smoothness_mean) - np.nanmin(final_smoothness_mean))\n",
    "plot_heatmap_top_filtered_percent(final_val_r2-final_smoothness_mean, ord1_smoothness_weights, ord2_smoothness_weights, \n",
    "             title='$R^2$ Normed - Smoothness Error', \n",
    "             xlabel='ord1_weight', \n",
    "             ylabel='ord2_weight')\n",
    "plt.show()\n",
    "\n",
    "# Plot R^2 over the smoothness strength scatter plot for all values\n",
    "plt.figure(figsize=(6, 4), dpi=150)\n",
    "plt.scatter(final_smoothness_mean.flatten(), final_val_r2.flatten(), c=(final_val_r2-final_smoothness_mean).flatten(), cmap='viridis', marker='x')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Smoothness Loss')\n",
    "plt.ylabel('Final Validation $R^2$')\n",
    "plt.title('Final Validation $R^2$ vs. Smoothness Loss for Different Ord1 and Ord2 Weights')\n",
    "plt.show()\n",
    "np.nanmax(final_val_r2-final_smoothness_mean), np.unravel_index(np.nanargmax(final_val_r2-final_smoothness_mean), final_val_r2.shape)\n",
    "idxs = np.unravel_index(np.nanargmax(final_val_r2-final_smoothness_mean), final_val_r2.shape)\n",
    "best_ord1 = ord1_smoothness_weights[idxs[0]]\n",
    "best_ord2 = ord2_smoothness_weights[idxs[1]]\n",
    "\n",
    "best_ord1, best_ord2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothness_weight_ord1=5\n",
    "# smoothness_weight_ord2=10\n",
    "smoothness_weight_ord1=5\n",
    "smoothness_weight_ord2=10\n",
    "smooth_model, _, _, _, global_mse_losses = train_nn(augmented_data, \n",
    "                                                    y_augmented_train, \n",
    "                                                    X_valid, \n",
    "                                                    y_valid, \n",
    "                                                    num_epochs=100, \n",
    "                                                    learning_rate=1e-4, \n",
    "                                                    weight_decay=1e-2, \n",
    "                                                    verbose=True,\n",
    "                                                    current_gdp_idx=-1,\n",
    "                                                    custom_loss=lambda x, y: custom_loss(x, y, smoothness_weight_ord1, smoothness_weight_ord2),\n",
    "                                                    seed=SEED)\n",
    "global_mse_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(False)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = smooth_model(x_valid_t).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_train = smooth_model(x_train_t).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_hf = smooth_model(x_hf_t).clone().detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hf_squeezed = {\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'data': x_hf_t.clone().detach().cpu().numpy().squeeze(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = lambda seed: train_nn(X_train, y_train, X_valid, y_valid, num_epochs=100, learning_rate=1e-4, weight_decay=1e-2, verbose=False, seed=seed)\n",
    "bootstrapped_results = results.bootstrap_ensemble(X_valid, y_valid, train_fn, n_ensembling=100, seed=SEED, device=get_device(False), other_pred_set=df_hf_squeezed)\n",
    "\n",
    "rsquared_ensemble = bootstrapped_results['rsquared_ensemble']\n",
    "smoothness_ensemble = bootstrapped_results['other_pred_smoothness']\n",
    "\n",
    "print(\"Basic Model R^2 Median: \", np.median(rsquared_ensemble))\n",
    "print(\"Basic Model Smoothness Median: \", np.median(smoothness_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the result by country and add 'Set' column\n",
    "y_pred_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_valid,\n",
    "    'country': preprocessor.country_valid,\n",
    "    'y_pred': y_pred,\n",
    "    'y_true': y_valid,\n",
    "    'Set': 'Validation'\n",
    "})\n",
    "\n",
    "y_pred_train_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train,\n",
    "    'country': preprocessor.country_train,\n",
    "    'y_pred': y_pred_train,\n",
    "    'y_true': y_train,\n",
    "    'Set': 'Training'\n",
    "})\n",
    "\n",
    "# Put together the train and the validation set\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n",
    "predictions_hf = pd.DataFrame({\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'y_pred': y_pred_hf,\n",
    "    'Set': 'High Frequency'\n",
    "})\n",
    "\n",
    "\n",
    "# Melting the dataframe for better plotting\n",
    "predictions_melted = predictions.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\", \"y_true\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "predictions_hf_melted = predictions_hf.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Function to plot data for the selected country\n",
    "def plot_by_country(selected_country):\n",
    "    filtered_data = predictions_melted[predictions_melted[\"country\"] == selected_country]\n",
    "    filtered_data_hf = predictions_hf_melted[predictions_hf_melted[\"country\"] == selected_country]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=filtered_data_hf,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True, palette=\"Set3\"\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        data=filtered_data,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Prediction vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend(title=\"Legend\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the plot function\n",
    "interact(plot_by_country, selected_country=dropdown)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesure smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model, basic_training_loss, basic_validation_loss, basic_validation_r_squared, basic_global_mse_losses = train_nn(X_train, \n",
    "                                                                        y_train, \n",
    "                                                                        X_valid, \n",
    "                                                                        y_valid, \n",
    "                                                                        num_epochs=100, \n",
    "                                                                        learning_rate=1e-4, \n",
    "                                                                        weight_decay=1e-2, \n",
    "                                                                        verbose=True,\n",
    "                                                                        seed=SEED)\n",
    "\n",
    "# Get the predictions\n",
    "basic_y_pred_hf = basic_model(torch.tensor(x_hf, dtype=torch.float32).to(device)).clone().detach().cpu().numpy().squeeze()\n",
    "basic_r2_score = results.r2_score(y_valid, basic_model(x_valid_t).clone().detach().cpu().numpy().squeeze())\n",
    "\n",
    "# R2 on hf\n",
    "adjusted_r2_score = results.r2_score(y_valid, model(x_valid_t).clone().detach().cpu().numpy().squeeze())\n",
    "\n",
    "print(f\"Basic R2 score: {basic_r2_score}\")\n",
    "print(f\"Adjusted R2 score: {adjusted_r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = lambda seed: train_nn(augmented_data, y_augmented_train, X_valid, y_valid, num_epochs=100, learning_rate=1e-4, weight_decay=1e-2, verbose=False, seed=seed, custom_loss=lambda x, y: custom_loss(x, y, smoothness_weight_ord1, smoothness_weight_ord2))\n",
    "smooth_bootstrapped_results = results.bootstrap_ensemble(X_valid, y_valid, train_fn, n_ensembling=100, seed=SEED, device=get_device(False), other_pred_set=df_hf_squeezed)\n",
    "\n",
    "smooth_rsquared_ensemble = smooth_bootstrapped_results['rsquared_ensemble']\n",
    "smooth_smoothness_ensemble = smooth_bootstrapped_results['other_pred_smoothness']\n",
    "\n",
    "print(\"Smooth Model R^2 Median: \", np.median(smooth_rsquared_ensemble))\n",
    "print(\"Smooth Model Smoothness Median: \", np.median(smooth_smoothness_ensemble))\n",
    "\n",
    "print(\"Basic Model R^2 Median: \", np.median(rsquared_ensemble))\n",
    "print(\"Basic Model Smoothness Median: \", np.median(smoothness_ensemble))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
