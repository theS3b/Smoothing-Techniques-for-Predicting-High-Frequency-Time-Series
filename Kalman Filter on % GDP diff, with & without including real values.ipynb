{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.neural_network import train_nn, get_device\n",
    "\n",
    "from models.KF import KF, apply_kalman_filter\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing_v2 import Preprocessing, get_gt_diff_logs\n",
    "import utils.results as results\n",
    "from utils.results import bootstrap_ensemble, interactive_plot_predictions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42\n",
    "\n",
    "plt.style.use('ieee.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9\n",
    "MODE = \"pct\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()\n",
    "data['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(epsilon=EPS, gdp_diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, x_hf = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, \n",
    "                                                                  mode=\"diff\", \n",
    "                                                                  take_log_diff_gdp=True,\n",
    "                                                                  gt_trend_removal=False, \n",
    "                                                                  keep_pca_components=180, \n",
    "                                                                  noisy_data_stds=[0.005, 0.05, 0.1], \n",
    "                                                                  add_encoded_month=False, \n",
    "                                                                  gt_data_transformations=[get_gt_diff_logs], other_params={'plot_pca': False})\n",
    "\n",
    "print(all_gts['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 15 Prediction Models with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MODELS = 15\n",
    "\n",
    "models = []\n",
    "for i in tqdm(range(N_MODELS)):\n",
    "    model, training_loss, validation_loss, validation_r_squared, mse_losses = train_nn(X_train, \n",
    "                                                                        y_train, \n",
    "                                                                        X_valid, \n",
    "                                                                        y_valid, \n",
    "                                                                        num_epochs=100, \n",
    "                                                                        learning_rate=1e-4, \n",
    "                                                                        weight_decay=1e-2, \n",
    "                                                                        verbose=i == N_MODELS-1,\n",
    "                                                                        seed=SEED+i)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "del model, training_loss, validation_loss, validation_r_squared, mse_losses\n",
    "ensembled_model = lambda x: torch.mean(torch.stack([model(x) for model in models]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(False)\n",
    "\n",
    "# Get the predictions\n",
    "x_valid = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "x_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_pred = models[0](x_valid).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_train = models[0](x_train).clone().detach().cpu().numpy().squeeze()\n",
    "\n",
    "# Associate the result by country and add 'Set' column\n",
    "y_pred_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_valid,\n",
    "    'country': preprocessor.country_valid,\n",
    "    'y_pred': y_pred,\n",
    "    'y_true': y_valid,\n",
    "    'Set': 'Validation'\n",
    "})\n",
    "\n",
    "y_pred_train_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train,\n",
    "    'country': preprocessor.country_train,\n",
    "    'y_pred': y_pred_train,\n",
    "    'y_true': y_train,\n",
    "    'Set': 'Training'\n",
    "})\n",
    "\n",
    "# Put together the train and the validation set\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n",
    "\n",
    "# Melting the dataframe for better plotting\n",
    "predictions_melted = predictions.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\", \"y_true\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot data for the selected country\n",
    "def plot_by_country(selected_country, predictions_melted=predictions_melted):\n",
    "    filtered_data = predictions_melted[predictions_melted[\"country\"] == selected_country]\n",
    "\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=filtered_data,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Prediction vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend(title=\"Legend\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the plot function\n",
    "interact(plot_by_country, selected_country=dropdown, predictions_melted=widgets.fixed(predictions_melted))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Kalman Filter on high frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to apply and plot the Kalman filter\n",
    "def apply_and_plot_kalman_filter(model, preprocessor, use_true_values, accurate_noise_var, accel_var):\n",
    "    kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness_loss = apply_kalman_filter(model, preprocessor, use_true_values=use_true_values, seed=SEED, accurate_noise_var=accurate_noise_var, accel_var=accel_var)\n",
    "\n",
    "    all_predictions_merged = pd.concat([predictions_melted[predictions_melted['Type'] != 'y_pred'], kf_predictions_melted, hf_data_melted], ignore_index=True)\n",
    "\n",
    "    # Create a dropdown widget for selecting the country\n",
    "    countries = all_predictions_merged[\"country\"].unique()\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=countries,\n",
    "        value=countries[0],\n",
    "        description='Country:'\n",
    "    )\n",
    "\n",
    "    mask_type = lambda df, type: df[df['Type'] == type]\n",
    "    print(f\"R2 for Kalman Filter: {kf_r2}, roughness: {kf_smoothness_loss[1]}\")\n",
    "    for col in ['y_pred', 'y_true']:\n",
    "        means = results.measure_smoothness(mask_type(all_predictions_merged, col)['Value'].values, mask_type(all_predictions_merged, col)['date'], mask_type(all_predictions_merged, col)['country'])\n",
    "        print(f\"Geometric mean of roughness metrics on {col}: {means[1]}\")\n",
    "\n",
    "    # Use the interact function to link the dropdown with the plot function\n",
    "    interact(plot_by_country, selected_country=dropdown, predictions_melted=widgets.fixed(all_predictions_merged))\n",
    "    plt.show()\n",
    "\n",
    "    # Return for paper plots\n",
    "    return all_predictions_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_and_plot_kalman_filter(ensembled_model, preprocessor, use_true_values=False, accurate_noise_var=0, accel_var=8e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing true values through the filter when available to correct the estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_and_plot_kalman_filter(ensembled_model, preprocessor, use_true_values=True, accurate_noise_var=5e-4, accel_var=8e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring noise variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = np.logspace(-12, 1, 100)\n",
    "\n",
    "metrics = pd.DataFrame(index=pd.MultiIndex.from_product([variances, range(N_MODELS)], names=['accel_var', 'model']), columns=['r2', 'roughness'])                \n",
    "ensemble_metrics = pd.DataFrame(columns=['r2', 'roughness'], index=pd.Index(variances, name='accel_var'))\n",
    "\n",
    "for accel_var_idx, acceleration_noise_variance in tqdm(enumerate(variances), total=len(variances)):\n",
    "    kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness_loss = apply_kalman_filter(ensembled_model, preprocessor, use_true_values=False, seed=SEED, accurate_noise_var=0, accel_var=acceleration_noise_variance)\n",
    "    ensemble_metrics.loc[acceleration_noise_variance, 'r2'] = kf_r2\n",
    "    ensemble_metrics.loc[acceleration_noise_variance, 'roughness'] = kf_smoothness_loss[1] # keep the geometric\n",
    "\n",
    "    for model_idx, model in tqdm(enumerate(models), total=len(models), leave=False):\n",
    "        kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness_loss = apply_kalman_filter(model, preprocessor, use_true_values=False, seed=SEED, accurate_noise_var=0, accel_var=acceleration_noise_variance)\n",
    "        \n",
    "        metrics.loc[(acceleration_noise_variance, model_idx), 'r2'] = kf_r2\n",
    "        metrics.loc[(acceleration_noise_variance, model_idx), 'roughness'] = kf_smoothness_loss[1] # keep the geometric mean only\n",
    "\n",
    "# Save the values for further use and plotting\n",
    "metrics.to_csv('paper_data/kalman_filter_metrics.csv')\n",
    "ensemble_metrics.to_csv('paper_data/kalman_filter_ensemble_metrics.csv')\n",
    "\n",
    "# Take the median values\n",
    "r2s = metrics.groupby('accel_var')['r2'].median().values\n",
    "smoothness_losses = metrics.groupby('accel_var')['roughness'].median().values\n",
    "\n",
    "optimal_metric = r2s - smoothness_losses\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(r\"$R^2$ and roughness for different acceleration noise variances\")\n",
    "plt.plot(variances, r2s, label=\"$R^2$\")\n",
    "plt.plot(variances, ensemble_metrics['r2'], label=\"Ensemble $R^2$\", color=\"purple\")\n",
    "plt.plot(variances, ensemble_metrics['roughness'], label=\"Ensemble roughness\", color=\"orange\")\n",
    "plt.plot(variances, smoothness_losses, label=\"Roughness\", color=\"green\")\n",
    "plt.plot(variances, optimal_metric, label=\"Optimal metric\", color=\"red\", linestyle=\"--\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Acceleration noise variance\")\n",
    "plt.ylabel(\"Metric value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_ACCEL_VAR = variances[np.argmax(optimal_metric)]\n",
    "print(f\"Optimal acceleration noise variance: {OPTIMAL_ACCEL_VAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_merged = apply_and_plot_kalman_filter(ensembled_model, preprocessor, use_true_values=False, accurate_noise_var=0, accel_var=OPTIMAL_ACCEL_VAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring noise variance values using true GDP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = np.logspace(-12, 2, 25)\n",
    "\n",
    "r2s = np.zeros((len(variances), len(variances)))\n",
    "smoothness_losses = np.zeros((len(variances), len(variances)))\n",
    "\n",
    "metrics = pd.DataFrame(index=pd.MultiIndex.from_product([variances, variances, range(N_MODELS)], names=['accel_var', 'true_values_var', 'model']), columns=['r2', 'roughness'])                \n",
    "ensemble_metrics = pd.DataFrame(columns=['r2', 'roughness'], index=pd.MultiIndex.from_product([variances, variances], names=['accel_var', 'true_values_var']))\n",
    "for accel_var_idx, acceleration_noise_variance in tqdm(enumerate(variances), total=len(variances)):\n",
    "    for true_values_var_idx, true_values_noise_variance in tqdm(enumerate(variances), total=len(variances), leave=False):\n",
    "        kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness_loss = apply_kalman_filter(ensembled_model, preprocessor, use_true_values=True, seed=SEED, accurate_noise_var=true_values_noise_variance, accel_var=acceleration_noise_variance)\n",
    "        ensemble_metrics.loc[(acceleration_noise_variance, true_values_noise_variance), 'r2'] = kf_r2\n",
    "        ensemble_metrics.loc[(acceleration_noise_variance, true_values_noise_variance), 'roughness'] = kf_smoothness_loss[1] # keep the geometric\n",
    "\n",
    "        for model_idx, model in tqdm(enumerate(models), total=len(models), leave=False):\n",
    "            kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness_loss = apply_kalman_filter(model, preprocessor, use_true_values=True, seed=SEED, accurate_noise_var=true_values_noise_variance, accel_var=acceleration_noise_variance)\n",
    "            \n",
    "            metrics.loc[(acceleration_noise_variance, true_values_noise_variance, model_idx), 'r2'] = kf_r2\n",
    "            metrics.loc[(acceleration_noise_variance, true_values_noise_variance, model_idx), 'roughness'] = kf_smoothness_loss[1] # keep the geometric mean only\n",
    "\n",
    "# Save the values for further use and plotting\n",
    "metrics.to_csv('paper_data/kalman_filter_metrics_with_true_values.csv')\n",
    "ensemble_metrics.to_csv('paper_data/kalman_filter_ensemble_metrics_with_true_values.csv')\n",
    "\n",
    "# take the medians across models\n",
    "r2s = metrics.reset_index(drop=False).groupby(['accel_var', 'true_values_var'])['r2'].median().values.reshape((len(variances), len(variances))).astype(float)\n",
    "smoothness_losses = metrics.reset_index(drop=False).groupby(['accel_var', 'true_values_var'])['roughness'].median().values.reshape((len(variances), len(variances))).astype(float)\n",
    "\n",
    "# combination of both\n",
    "optimal_metric = r2s - smoothness_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_variance_heatmap(r2s, smoothness_losses, optimal_metric, variances, file_path=None):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4), sharey=True, sharex=True, dpi=300)\n",
    "    fig.suptitle(\"Metrics for different noise variances\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_box_aspect(1)\n",
    "\n",
    "    ticklabels = [f'{v:.2e}' if i % 2 == 0 else '' for i, v in enumerate(variances)] # show every second label\n",
    "\n",
    "    # heatmap for R2\n",
    "    sns.heatmap(r2s, annot=False, fmt=\".2f\", xticklabels=ticklabels, yticklabels=ticklabels, cmap=\"coolwarm\", ax=axes[0])\n",
    "    axes[0].set_title(r\"$R^2$\")\n",
    "    axes[0].set_xlabel(r\"var_\\text{theory}\")\n",
    "    axes[0].set_ylabel(r\"var_\\text{true}\")\n",
    "\n",
    "    # heatmap for smoothness\n",
    "    sns.heatmap(-smoothness_losses, annot=False, fmt=\".2f\", xticklabels=ticklabels, yticklabels=ticklabels, cmap=\"coolwarm\", ax=axes[1])\n",
    "    axes[1].set_title(r\"$-e_s(\\boldsymbol{\\hat{y}}_h)$\")\n",
    "    axes[1].set_xlabel(r\"$var_\\text{theory}$\")\n",
    "    axes[1].set_ylabel(r\"$var_\\text{true}$\")\n",
    "\n",
    "    # heatmap for the optimal metric\n",
    "    sns.heatmap(optimal_metric, annot=False, fmt=\".2f\", xticklabels=ticklabels, yticklabels=ticklabels, cmap=\"coolwarm\", ax=axes[2])\n",
    "    axes[2].set_title(r\"$R^2 - e_s(\\boldsymbol{\\hat{y}}_h)$\")\n",
    "    axes[2].set_xlabel(r\"$var_\\text{theory}$\")\n",
    "    axes[2].set_ylabel(r\"$var_\\text{true}$\")\n",
    "\n",
    "    if file_path:\n",
    "        plt.savefig(file_path, bbox_inches='tight')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "noise_variance_heatmap(r2s, smoothness_losses, optimal_metric, variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the optimal parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_indices = np.unravel_index(np.argmax(optimal_metric, axis=None), optimal_metric.shape)\n",
    "OPTIMAL_ACCURATE_NOISE_VAR = variances[optimal_indices[1]]\n",
    "OPTIMAL_ACCEL_VAR = variances[optimal_indices[0]]\n",
    "\n",
    "print(f\"Optimal acceleration noise variance: {OPTIMAL_ACCEL_VAR:.2e}\")\n",
    "print(f\"Optimal true GDP noise variance: {OPTIMAL_ACCURATE_NOISE_VAR:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_merged_true_value = apply_and_plot_kalman_filter(model, preprocessor, use_true_values=True, accurate_noise_var=OPTIMAL_ACCURATE_NOISE_VAR, accel_var=OPTIMAL_ACCEL_VAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_COUNTRY = 'Germany'\n",
    "\n",
    "# Prepare the data\n",
    "type_mapping = {\n",
    "    'y_pred': 'Neural network predictions',\n",
    "    'y_kf': 'Filter predictions',\n",
    "    'y_true': 'True values'\n",
    "}\n",
    "\n",
    "plot_data = predictions_merged.copy()\n",
    "plot_data['Type'] = plot_data['Type'].apply(lambda x: type_mapping[x])\n",
    "true_values = predictions_merged_true_value.loc[predictions_merged_true_value['Type'] == 'y_kf'].assign(Type='Filter predictions with true values')\n",
    "plot_data = pd.concat([plot_data, true_values], ignore_index=True)\n",
    "\n",
    "# Cutoff date\n",
    "cutoff_date = (preprocessor.dates_valid.values[0] - preprocessor.dates_train.values[-1]) / 2 + preprocessor.dates_train.values[-1]\n",
    "\n",
    "filtered_data = plot_data[(plot_data[\"country\"] == SELECTED_COUNTRY) & (plot_data[\"Type\"] != \"Neural network predictions\")]\n",
    "high_freq_predictions = plot_data[(plot_data[\"country\"] == SELECTED_COUNTRY) & (plot_data[\"Type\"] == \"Neural network predictions\")]\n",
    "\n",
    "plt.style.use('ieee.mplstyle')\n",
    "plt.figure(figsize=(8, 5), dpi=300)\n",
    "\n",
    "plt.plot(high_freq_predictions[\"date\"], \n",
    "         high_freq_predictions[\"Value\"],\n",
    "         label=\"Neural network predictions\",\n",
    "         color=\"#1F77B4\",\n",
    "         linewidth=1,\n",
    "         marker=',', \n",
    "         alpha=0.6)\n",
    "\n",
    "sns.lineplot(\n",
    "    data=filtered_data,\n",
    "    x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True, markersize=3.8\n",
    ")\n",
    "plt.title(f\"Kalman filter prediction vs true values for {SELECTED_COUNTRY}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Normalised prediction\")\n",
    "plt.axvline(cutoff_date, color='red', linestyle='--', label='Start of Validation Set', linewidth=1)\n",
    "plt.legend(\n",
    "    loc=\"upper center\",  # Position the legend in the center\n",
    "    bbox_to_anchor=(0.5, -0.15),  # Center it below the plot\n",
    "    ncol=3,  # Arrange items in a single row\n",
    "    frameon=False  # Optional: Remove the legend frame\n",
    ")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(results.OUTPUT_DATA_PATH + f'KF_{SELECTED_COUNTRY.replace(' ', '_')}.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ieee.mplstyle')\n",
    "noise_variance_heatmap(r2s, smoothness_losses, optimal_metric, variances, file_path=results.OUTPUT_DATA_PATH + 'KF_heatmap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ieee.mplstyle')\n",
    "\n",
    "metrics = pd.read_csv('paper_data/kalman_filter_metrics.csv', index_col=[0, 1])\n",
    "ensemble_metrics = pd.read_csv('paper_data/kalman_filter_ensemble_metrics.csv', index_col=0)\n",
    "metrics_true_values = pd.read_csv('paper_data/kalman_filter_metrics_with_true_values.csv', index_col=[0, 1, 2])\n",
    "ensemble_metrics_true_values = pd.read_csv('paper_data/kalman_filter_ensemble_metrics_with_true_values.csv', index_col=[0, 1])\n",
    "\n",
    "c = metrics['r2'].values - metrics['roughness'].values\n",
    "ce = ensemble_metrics['r2'].values - ensemble_metrics['roughness'].values\n",
    "c_tv = metrics_true_values['r2'].values - metrics_true_values['roughness'].values\n",
    "ce_tv = ensemble_metrics_true_values['r2'].values - ensemble_metrics_true_values['roughness'].values\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(8, 4), dpi=150)\n",
    "\n",
    "# Scatter plots\n",
    "\n",
    "sc_tv = plt.scatter(metrics_true_values['roughness'].values, metrics_true_values['r2'].values,\n",
    "                    c=c_tv, cmap='Blues', marker='o', s=20, alpha=0.7, label='Batch Size 6')\n",
    "\n",
    "sc = plt.scatter(metrics['roughness'].values, metrics['r2'].values,\n",
    "                    c=c, cmap='Reds', marker='o', s=20, alpha=0.7, label='Batch Size 4')\n",
    "\n",
    "sce = plt.scatter(ensemble_metrics['roughness'].values, ensemble_metrics['r2'].values,\n",
    "                    c=ce, cmap='Reds', marker='x', s=20, alpha=0.7, label='Batch Size 4')\n",
    "\n",
    "sce_tv = plt.scatter(ensemble_metrics_true_values['roughness'].values, ensemble_metrics_true_values['r2'].values,\n",
    "                    c=ce_tv, cmap='Blues', marker='x', s=20, alpha=0.7, label='Batch Size 6')\n",
    "\n",
    "plt.scatter(0.6549, 0.1512, label='Median Reference Model', color='green', marker='o', s=25)\n",
    "plt.scatter(0.4968, 0.4515, label='Ensembling on Reference Model', color='green', marker='x', s=25)\n",
    "\n",
    "# Add labels, title, and grid\n",
    "plt.xlabel(r'Smoothness Loss $e_s(\\boldsymbol{\\hat{y}_h})$')\n",
    "plt.ylabel('Validation $R^2$')\n",
    "plt.title(r'Validation $R^2$ vs. High-Frequency Smoothness Loss $e_s(\\boldsymbol{\\hat{y}_h})$')\n",
    "\n",
    "# Custom legend with color markers for maximum c values\n",
    "plt.legend(handles=[\n",
    "    plt.Line2D([0], [0], marker='s', color='w', label='Kalman Filter on reference model',\n",
    "               markerfacecolor='red', markeredgecolor='red', markersize=7),\n",
    "    plt.Line2D([0], [0], marker='s', color='w', label=r'Kalman Filter on reference model with true values',\n",
    "               markerfacecolor='blue', markeredgecolor='blue', markersize=7),\n",
    "    plt.Line2D([0], [0], marker='s', color='w', label='Reference Model',\n",
    "               markerfacecolor='green', markeredgecolor='green', markersize=7),\n",
    "               \n",
    "plt.Line2D([0], [0], marker='x', color='w', label='Ensembled Predictions',\n",
    "               markerfacecolor='black', markeredgecolor='black', markersize=7),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Median Model',\n",
    "               markerfacecolor='black', markeredgecolor='black', markersize=7),\n",
    "], loc='center', bbox_to_anchor=(0.5, -0.3), ncol=2, frameon=False),\n",
    "\n",
    "#plt.ylim(0, 0.55)\n",
    "#plt.xlim(0, 0.7)\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig('data/output_for_paper/kalman_filter_scatter.pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
