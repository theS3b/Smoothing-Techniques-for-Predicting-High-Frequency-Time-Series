{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.neural_network import train_nn, get_device\n",
    "\n",
    "from models.KF import KF, apply_kalman_filter\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing_v2 import Preprocessing, get_gt_diff_logs\n",
    "import utils.results as results\n",
    "from utils.results import bootstrap_ensemble, interactive_plot_predictions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9\n",
    "MODE = \"pct\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()\n",
    "data['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(epsilon=EPS, gdp_diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, x_hf = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, \n",
    "                                                                  mode=\"diff\", \n",
    "                                                                  take_log_diff_gdp=True,\n",
    "                                                                  gt_trend_removal=False, \n",
    "                                                                  keep_pca_components=180, \n",
    "                                                                  noisy_data_stds=[0.005, 0.05, 0.1], \n",
    "                                                                  add_encoded_month=False, \n",
    "                                                                  gt_data_transformations=[get_gt_diff_logs], other_params={'plot_pca': False})\n",
    "\n",
    "print(all_gts['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss, validation_loss, validation_r_squared, mse_losses = train_nn(X_train, \n",
    "                                                                       y_train, \n",
    "                                                                       X_valid, \n",
    "                                                                       y_valid, \n",
    "                                                                       num_epochs=100, \n",
    "                                                                       learning_rate=1e-4, \n",
    "                                                                       weight_decay=1e-2, \n",
    "                                                                       verbose=True,\n",
    "                                                                       seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(False)\n",
    "\n",
    "# Get the predictions\n",
    "x_valid = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "x_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_pred = model(x_valid).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_train = model(x_train).clone().detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the result by country and add 'Set' column\n",
    "y_pred_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_valid,\n",
    "    'country': preprocessor.country_valid,\n",
    "    'y_pred': y_pred,\n",
    "    'y_true': y_valid,\n",
    "    'Set': 'Validation'\n",
    "})\n",
    "\n",
    "y_pred_train_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train,\n",
    "    'country': preprocessor.country_train,\n",
    "    'y_pred': y_pred_train,\n",
    "    'y_true': y_train,\n",
    "    'Set': 'Training'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together the train and the validation set\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n",
    "\n",
    "# Melting the dataframe for better plotting\n",
    "predictions_melted = predictions.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\", \"y_true\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Function to plot data for the selected country\n",
    "def plot_by_country(selected_country, predictions_melted=predictions_melted):\n",
    "    filtered_data = predictions_melted[predictions_melted[\"country\"] == selected_country]\n",
    "\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=filtered_data,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Prediction vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend(title=\"Legend\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the plot function\n",
    "interact(plot_by_country, selected_country=dropdown, predictions_melted=widgets.fixed(predictions_melted))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Kalman Filter on high frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to apply and plot the Kalman filter\n",
    "def apply_and_plot_kalman_filter(model, preprocessor, use_true_values, accurate_noise_var, accel_var):\n",
    "    kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness = apply_kalman_filter(model, preprocessor, use_true_values=use_true_values, seed=SEED, accurate_noise_var=accurate_noise_var, accel_var=accel_var)\n",
    "\n",
    "    all_predictions_merged = pd.concat([predictions_melted[predictions_melted['Type'] != 'y_pred'], kf_predictions_melted, hf_data_melted], ignore_index=True)\n",
    "\n",
    "    # Create a dropdown widget for selecting the country\n",
    "    countries = all_predictions_merged[\"country\"].unique()\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=countries,\n",
    "        value=countries[0],\n",
    "        description='Country:'\n",
    "    )\n",
    "\n",
    "    mask_type = lambda df, type: df[df['Type'] == type]\n",
    "    print(f\"R2 for Kalman Filter: {kf_r2}, smoothness: {kf_smoothness[1]}\")\n",
    "    for col in ['y_pred', 'y_true']:\n",
    "        means = results.measure_smoothness(mask_type(all_predictions_merged, col)['Value'].values, mask_type(all_predictions_merged, col)['date'], mask_type(all_predictions_merged, col)['country'])\n",
    "        print(f\"Geometric mean of smoothness metrics on {col}: {means[1]}\")\n",
    "\n",
    "    # Use the interact function to link the dropdown with the plot function\n",
    "    interact(plot_by_country, selected_country=dropdown, predictions_melted=widgets.fixed(all_predictions_merged))\n",
    "    plt.show()\n",
    "\n",
    "    # Return for paper plots\n",
    "    return all_predictions_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_and_plot_kalman_filter(model, preprocessor, use_true_values=False, accurate_noise_var=0, accel_var=8e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing true values through the filter when available to correct the estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_and_plot_kalman_filter(model, preprocessor, use_true_values=True, accurate_noise_var=5e-4, accel_var=8e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring noise variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = np.logspace(-12, 1, 100)\n",
    "\n",
    "r2s = np.zeros(len(variances))\n",
    "smoothness_losses = np.zeros(len(variances))\n",
    "                        \n",
    "for accel_var_idx, acceleration_noise_variance in tqdm(enumerate(variances), total=len(variances)):\n",
    "    kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness = apply_kalman_filter(model, preprocessor, use_true_values=False, seed=SEED, accurate_noise_var=0, accel_var=acceleration_noise_variance)\n",
    "    r2s[accel_var_idx] = kf_r2\n",
    "    smoothness_losses[accel_var_idx] = kf_smoothness[1] # keep the geometric mean only\n",
    "\n",
    "optimal_metric = (r2s - np.min(r2s)) / (np.max(r2s) - np.min(r2s)) - (smoothness_losses - np.min(smoothness_losses)) / (np.max(smoothness_losses) - np.min(smoothness_losses))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"R2 and smoothness loss for different acceleration noise variances\")\n",
    "plt.plot(variances, r2s, label=\"R2\")\n",
    "plt.plot(variances, smoothness_losses, label=\"Smoothness\")\n",
    "plt.plot(variances, optimal_metric, label=\"Optimal metric\", color=\"red\", linestyle=\"--\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Acceleration noise variance\")\n",
    "plt.ylabel(\"Metric value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_ACCEL_VAR = variances[np.argmax(optimal_metric)]\n",
    "print(f\"Optimal acceleration noise variance: {OPTIMAL_ACCEL_VAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_merged = apply_and_plot_kalman_filter(model, preprocessor, use_true_values=False, accurate_noise_var=0, accel_var=OPTIMAL_ACCEL_VAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring noise variance values using true GDP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = np.logspace(-12, 2, 25)\n",
    "\n",
    "r2s = np.zeros((len(variances), len(variances)))\n",
    "smoothness_losses = np.zeros((len(variances), len(variances)))\n",
    "                        \n",
    "for accel_var_idx, acceleration_noise_variance in tqdm(enumerate(variances), total=len(variances)):\n",
    "    for true_gdp_var_idx, true_gdp_noise_variance in tqdm(enumerate(variances), total=len(variances), leave=False):\n",
    "        kf_predictions_melted, hf_data_melted, kf_r2, kf_smoothness = apply_kalman_filter(model, preprocessor, use_true_values=True, seed=SEED, accurate_noise_var=true_gdp_noise_variance, accel_var=acceleration_noise_variance)\n",
    "        r2s[accel_var_idx, true_gdp_var_idx] = kf_r2\n",
    "        smoothness_losses[accel_var_idx, true_gdp_var_idx] = kf_smoothness[1] # keep the geometric mean only\n",
    "        # x is the acceleration variance, y is the true gdp variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combination of both\n",
    "normalized_r2s = (r2s - r2s.min()) / (r2s.max() - r2s.min())\n",
    "normalized_smoothness_losses = (smoothness_losses - smoothness_losses.min()) / (smoothness_losses.max() - smoothness_losses.min())\n",
    "optimal_metric = normalized_r2s - normalized_smoothness_losses\n",
    "\n",
    "def noise_variance_heatmap(r2s, smoothness_losses, optimal_metric, variances, file_path=None):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4), sharey=True, sharex=True, dpi=300)\n",
    "    fig.suptitle(\"Metrics for different noise variances\", fontsize=16)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_box_aspect(1)\n",
    "\n",
    "    ticklabels = [f'{v:.2e}' if i % 2 == 0 else '' for i, v in enumerate(variances)] # show every second label\n",
    "\n",
    "    # heatmap for R2\n",
    "    sns.heatmap(r2s, annot=False, fmt=\".2f\", xticklabels=ticklabels, yticklabels=ticklabels, cmap=\"coolwarm\", ax=axes[0])\n",
    "    axes[0].set_title(\"R^2\")\n",
    "    axes[0].set_xlabel(\"Theoretical Noise Variance\")\n",
    "    axes[0].set_ylabel(\"True values Noise Variance\")\n",
    "\n",
    "    # heatmap for smoothness\n",
    "    sns.heatmap(-smoothness_losses, annot=False, fmt=\".2f\", xticklabels=ticklabels, yticklabels=ticklabels, cmap=\"coolwarm\", ax=axes[1])\n",
    "    axes[1].set_title(\"Negative smoothness\")\n",
    "    axes[1].set_xlabel(\"Theoretical Noise Variance\")\n",
    "    axes[1].set_ylabel(\"True values Noise Variance\")\n",
    "\n",
    "    # heatmap for the optimal metric\n",
    "    sns.heatmap(optimal_metric, annot=False, fmt=\".2f\", xticklabels=ticklabels, yticklabels=ticklabels, cmap=\"coolwarm\", ax=axes[2])\n",
    "    axes[2].set_title(\"Normalized R2 - Normalized Smoothness\")\n",
    "    axes[2].set_xlabel(\"Theoretical Noise Variance\")\n",
    "    axes[2].set_ylabel(\"True values Noise Variance\")\n",
    "\n",
    "    if file_path:\n",
    "        plt.savefig(file_path, bbox_inches='tight')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "noise_variance_heatmap(r2s, smoothness_losses, optimal_metric, variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the optimal parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_indices = np.unravel_index(np.argmax(optimal_metric, axis=None), optimal_metric.shape)\n",
    "OPTIMAL_ACCURATE_NOISE_VAR = variances[optimal_indices[1]]\n",
    "OPTIMAL_ACCEL_VAR = variances[optimal_indices[0]]\n",
    "\n",
    "print(f\"Optimal acceleration noise variance: {OPTIMAL_ACCEL_VAR:.2e}\")\n",
    "print(f\"Optimal true GDP noise variance: {OPTIMAL_ACCURATE_NOISE_VAR:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_merged_true_value = apply_and_plot_kalman_filter(model, preprocessor, use_true_values=True, accurate_noise_var=OPTIMAL_ACCURATE_NOISE_VAR, accel_var=OPTIMAL_ACCEL_VAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use('ieee.mplstyle') # TODO use latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_COUNTRY = 'Switzerland'\n",
    "\n",
    "# Prepare the data\n",
    "type_mapping = {\n",
    "    'y_pred': 'Neural network predictions',\n",
    "    'y_kf': 'Filter predictions',\n",
    "    'y_true': 'True values'\n",
    "}\n",
    "\n",
    "plot_data = predictions_merged.copy()\n",
    "plot_data['Type'] = plot_data['Type'].apply(lambda x: type_mapping[x])\n",
    "true_values = predictions_merged_true_value.loc[predictions_merged_true_value['Type'] == 'y_kf'].assign(Type='Filter predictions with true values')\n",
    "plot_data = pd.concat([plot_data, true_values], ignore_index=True)\n",
    "\n",
    "# Cutoff date\n",
    "cutoff_date = (preprocessor.dates_valid.values[0] - preprocessor.dates_train.values[-1]) / 2 + preprocessor.dates_train.values[-1]\n",
    "\n",
    "filtered_data = plot_data[(plot_data[\"country\"] == SELECTED_COUNTRY) & (plot_data[\"Type\"] != \"Neural network predictions\")]\n",
    "high_freq_predictions = plot_data[(plot_data[\"country\"] == SELECTED_COUNTRY) & (plot_data[\"Type\"] == \"Neural network predictions\")]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(high_freq_predictions[\"date\"], \n",
    "         high_freq_predictions[\"Value\"],\n",
    "         label=\"Neural network predictions\",\n",
    "         color=\"#1F77B4\",\n",
    "         linewidth=1,\n",
    "         marker=',', \n",
    "         alpha=0.6)\n",
    "\n",
    "sns.lineplot(\n",
    "    data=filtered_data,\n",
    "    x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True\n",
    ")\n",
    "plt.title(f\"Prediction vs true values for {SELECTED_COUNTRY}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Normalised YoY Log GDP Growth\")\n",
    "plt.axvline(cutoff_date, color='red', linestyle='--', label='Start of Validation Set', linewidth=1.5)\n",
    "plt.legend(\n",
    "    loc=\"upper center\",  # Position the legend in the center\n",
    "    bbox_to_anchor=(0.5, -0.15),  # Center it below the plot\n",
    "    ncol=3,  # Arrange items in a single row\n",
    "    frameon=False  # Optional: Remove the legend frame\n",
    ")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.ylim([-4.75, 5]) # Set the y-axis limits (adapted to Switzerland)\n",
    "plt.xlim([datetime(2018, 1, 1), datetime(2025, 1, 1)]) # Show data from 2018 to 2025\n",
    "plt.savefig(results.OUTPUT_DATA_PATH + 'KF_Switzerland.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_variance_heatmap(r2s, smoothness_losses, optimal_metric, variances, file_path=results.OUTPUT_DATA_PATH + 'KF_heatmap.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
