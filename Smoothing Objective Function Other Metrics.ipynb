{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from utils.neural_network import train_nn, get_device, batch_data_by_country\n",
    "\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing_v2 import Preprocessing, get_gt_diff_logs\n",
    "import utils.results as results\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9\n",
    "MODE = \"pct\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()\n",
    "data['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(epsilon=EPS, gdp_diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, x_hf = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, \n",
    "                                                                  mode=\"diff\", \n",
    "                                                                  take_log_diff_gdp=True,\n",
    "                                                                  gt_trend_removal=False, \n",
    "                                                                  keep_pca_components=180, \n",
    "                                                                  noisy_data_stds=[0.005, 0.05, 0.1], \n",
    "                                                                  add_encoded_month=False, \n",
    "                                                                  gt_data_transformations=[get_gt_diff_logs], other_params={'plot_pca': False})\n",
    "\n",
    "x_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "x_valid_t = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "y_valid_t = torch.tensor(y_valid, dtype=torch.float32).to(device)\n",
    "x_hf_t = torch.tensor(x_hf, dtype=torch.float32).to(device)\n",
    "\n",
    "print(all_gts['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.dates_high_freq\n",
    "preprocessor.country_high_freq\n",
    "\n",
    "df_hf = pd.DataFrame({\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'data': [x_hf[i] for i in range(len(x_hf))]\n",
    "})\n",
    "\n",
    "df_x_train = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train.copy(),\n",
    "    'country': preprocessor.country_train.copy(),\n",
    "    'data': [X_train[i].copy() for i in range(len(X_train))],\n",
    "    'y_true': [y_train[i].copy() for i in range(len(y_train))]\n",
    "})\n",
    "\n",
    "df_hf['date'] = pd.to_datetime(df_hf['date'])\n",
    "df_x_train['date'] = pd.to_datetime(df_x_train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "def compute_smoothness_loss(y_pred, smoothness_weight_ord1=0.33, smoothness_weight_ord2=0.33):\n",
    "    if y_pred.shape[1] > 1:\n",
    "        # Compute the loss related to smoothness\n",
    "        smoothness_loss_ord1 = 0\n",
    "        for i in range(1, 4):\n",
    "            # Divide by 3 because dt is 1/3 month, and by 4 because we have 4 time points and we average them\n",
    "            smoothness_loss_ord1 += torch.linalg.norm((y_pred[:, i, :] - y_pred[:, i - 1, :])) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        # Constraint on the double derivative\n",
    "        smoothness_loss_ord2 = 0\n",
    "        for i in range(2, 4):\n",
    "            smoothness_loss_ord2 += torch.linalg.norm(y_pred[:, i, :] - 2 * y_pred[:, i - 1, :] + y_pred[:, i - 2, :]) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        return smoothness_loss_ord1 * smoothness_weight_ord1 + smoothness_loss_ord2 * smoothness_weight_ord2\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")\n",
    "\n",
    "def custom_loss(y_pred, y_true, smoothness_weight_ord1=0.33, smoothness_weight_ord2=0.33):\n",
    "    # Compute usual MSE on actual GDP values\n",
    "    mse = MSELoss()\n",
    "\n",
    "    if y_pred.shape[1] > 1:\n",
    "        prediction_loss = mse(y_pred[:, -1, :], y_true)  # First column is the actual GDP\n",
    "\n",
    "        smooth_loss = compute_smoothness_loss(y_pred, smoothness_weight_ord1, smoothness_weight_ord2)\n",
    "\n",
    "        return prediction_loss + smooth_loss\n",
    "    elif y_pred.shape[1] == 1:\n",
    "        return mse(y_pred, y_true)\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data, filtered_countries, filtered_dates, y_augmented_train, _ = batch_data_by_country(df_x_train, \n",
    "                                                                                              df_hf, \n",
    "                                                                                              3, \n",
    "                                                                                              0, \n",
    "                                                                                              ['y_true'], \n",
    "                                                                                              False)\n",
    "\n",
    "y_augmented_train = np.array(y_augmented_train).squeeze().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothness_weight_ord1=5\n",
    "smoothness_weight_ord2=10\n",
    "model, training_loss, validation_loss, validation_r_squared, global_mse_losses = train_nn(augmented_data, \n",
    "                                                                        y_augmented_train, \n",
    "                                                                        X_valid, \n",
    "                                                                        y_valid, \n",
    "                                                                        num_epochs=100, \n",
    "                                                                        learning_rate=1e-4, \n",
    "                                                                        weight_decay=1e-2, \n",
    "                                                                        verbose=True,\n",
    "                                                                        current_gdp_idx=-1,\n",
    "                                                                        custom_loss=lambda x, y: custom_loss(x, y, smoothness_weight_ord1, smoothness_weight_ord2),\n",
    "                                                                        seed=SEED)\n",
    "global_mse_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(False)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = model(x_valid_t).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_train = model(x_train_t).clone().detach().cpu().numpy().squeeze()\n",
    "y_pred_hf = model(x_hf_t).clone().detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Associate the result by country and add 'Set' column\n",
    "y_pred_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_valid,\n",
    "    'country': preprocessor.country_valid,\n",
    "    'y_pred': y_pred,\n",
    "    'y_true': y_valid,\n",
    "    'Set': 'Validation'\n",
    "})\n",
    "\n",
    "y_pred_train_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train,\n",
    "    'country': preprocessor.country_train,\n",
    "    'y_pred': y_pred_train,\n",
    "    'y_true': y_train,\n",
    "    'Set': 'Training'\n",
    "})\n",
    "\n",
    "# Put together the train and the validation set\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n",
    "predictions_hf = pd.DataFrame({\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'y_pred': y_pred_hf,\n",
    "    'Set': 'High Frequency'\n",
    "})\n",
    "\n",
    "\n",
    "# Melting the dataframe for better plotting\n",
    "predictions_melted = predictions.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\", \"y_true\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "predictions_hf_melted = predictions_hf.melt(\n",
    "    id_vars=[\"date\", \"country\", \"Set\"],\n",
    "    value_vars=[\"y_pred\"], \n",
    "    var_name=\"Type\", value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Function to plot data for the selected country\n",
    "def plot_by_country(selected_country):\n",
    "    filtered_data = predictions_melted[predictions_melted[\"country\"] == selected_country]\n",
    "    filtered_data_hf = predictions_hf_melted[predictions_hf_melted[\"country\"] == selected_country]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=filtered_data_hf,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True, palette=\"Set3\"\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        data=filtered_data,\n",
    "        x=\"date\", y=\"Value\", hue=\"Type\", errorbar = None, style=\"Set\", markers=True\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Prediction vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend(title=\"Legend\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the plot function\n",
    "interact(plot_by_country, selected_country=dropdown)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other smoothness Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch by country\n",
    "other_augmented_data, other_filtered_countries, other_filtered_dates, other_y_augmented_train, _ = batch_data_by_country(df_x_train, \n",
    "                                                                                                    df_hf, \n",
    "                                                                                                    5, \n",
    "                                                                                                    0, \n",
    "                                                                                                    ['y_true'], \n",
    "                                                                                                    False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "def compute_smoothness_loss(y_pred, smoothness_weight_ord1=0.33, smoothness_weight_ord2=0.33):\n",
    "    if y_pred.shape[1] > 1:\n",
    "        # Compute the loss related to smoothness\n",
    "        smoothness_loss_ord1 = 0\n",
    "        for i in range(1, y_pred.shape[1]):\n",
    "            # Divide by 3 because dt is 1/3 month, and by 4 because we have 4 time points and we average them\n",
    "            smoothness_loss_ord1 += torch.linalg.norm((y_pred[:, i, :] - y_pred[:, i - 1, :])) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        # Constraint on the double derivative\n",
    "        smoothness_loss_ord2 = 0\n",
    "        for i in range(2, y_pred.shape[1]):\n",
    "            smoothness_loss_ord2 += torch.linalg.norm(y_pred[:, i, :] - 2 * y_pred[:, i - 1, :] + y_pred[:, i - 2, :]) ** 2 / (3 * 4 * y_pred.shape[0])\n",
    "\n",
    "        return smoothness_loss_ord1 * smoothness_weight_ord1 + smoothness_loss_ord2 * smoothness_weight_ord2\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")\n",
    "    \n",
    "# def holder_exponent(series):\n",
    "#     \"\"\"Estimate the HÃ¶lder exponent.\"\"\"\n",
    "#     diff = torch.abs(torch.diff(series))\n",
    "#     return -torch.log(torch.mean(diff)) / torch.log(torch.tensor(len(series)))\n",
    "\n",
    "def std_first_derivative(series):\n",
    "    \"\"\"Standard deviation of the first derivative (finite differences).\"\"\"\n",
    "    diff = torch.diff(series)\n",
    "    return torch.std(diff)\n",
    "\n",
    "def mean_abs_first_difference(series):\n",
    "    \"\"\"Mean absolute first difference.\"\"\"\n",
    "    diff = torch.diff(series)\n",
    "    return torch.mean(torch.abs(diff))\n",
    "\n",
    "def std_second_derivative(series):\n",
    "    \"\"\"Variance or standard deviation of the second derivative.\"\"\"\n",
    "    second_diff = torch.diff(series, n=2)\n",
    "    return torch.std(second_diff)\n",
    "\n",
    "def total_variation(series):\n",
    "    \"\"\"Total variation.\"\"\"\n",
    "    diff = torch.diff(series)\n",
    "    return torch.mean(torch.abs(diff))\n",
    "\n",
    "smooth_functions = [std_first_derivative, mean_abs_first_difference, std_second_derivative, total_variation]\n",
    "\n",
    "def custom_loss(y_pred, y_true, weights=[1, 1, 1, 1, 1, 1]):\n",
    "    # Compute usual MSE on actual GDP values\n",
    "    mse = MSELoss()\n",
    "\n",
    "    if y_pred.shape[1] > 1:\n",
    "        prediction_loss = mse(y_pred[:, -1, :].squeeze(), y_true.squeeze())  # First column is the actual GDP\n",
    "\n",
    "        smooth_loss = torch.log1p(compute_smoothness_loss(y_pred, weights[0], weights[1]))\n",
    "\n",
    "        for i, f in enumerate(smooth_functions):\n",
    "            smooth_loss += torch.log1p(f(y_pred.squeeze()) * weights[i + 2])\n",
    "\n",
    "        smooth_loss = smooth_loss / (len(smooth_functions) + 2)\n",
    "        smooth_loss = torch.expm1(smooth_loss) \n",
    "\n",
    "        return prediction_loss + smooth_loss\n",
    "    elif y_pred.shape[1] == 1:\n",
    "        return mse(y_pred, y_true)\n",
    "    else:\n",
    "        raise ValueError(\"Input shape not supported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_augmented_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_smooth_model, ultra_smooth_training_loss, ultra_smooth_validation_loss, ultra_smooth_validation_r_squared, ultra_smooth_global_mse_losses = train_nn(other_augmented_data, \n",
    "                                                                        other_y_augmented_train, \n",
    "                                                                        X_valid, \n",
    "                                                                        y_valid, \n",
    "                                                                        num_epochs=100, \n",
    "                                                                        learning_rate=1e-4, \n",
    "                                                                        weight_decay=1e-2, \n",
    "                                                                        verbose=True,\n",
    "                                                                        current_gdp_idx=-1,\n",
    "                                                                        custom_loss=lambda x, y: custom_loss(x, y, [smoothness_weight_ord1, smoothness_weight_ord2, 1, 1, 1, 1]),\n",
    "                                                                        seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_smooth_y_pred_valid = ultra_smooth_model(x_valid_t).clone().detach().cpu().numpy().squeeze()\n",
    "ultra_smooth_y_pred_train = ultra_smooth_model(x_train_t).clone().detach().cpu().numpy().squeeze()\n",
    "ultra_smooth_y_pred_hf = ultra_smooth_model(x_hf_t).clone().detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesure smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model, basic_training_loss, basic_validation_loss, basic_validation_r_squared, basic_global_mse_losses = train_nn(X_train, \n",
    "                                                                        y_train, \n",
    "                                                                        X_valid, \n",
    "                                                                        y_valid, \n",
    "                                                                        num_epochs=100, \n",
    "                                                                        learning_rate=1e-4, \n",
    "                                                                        weight_decay=1e-2, \n",
    "                                                                        verbose=True,\n",
    "                                                                        seed=SEED)\n",
    "\n",
    "# Get the predictions\n",
    "basic_y_pred_hf = basic_model(torch.tensor(x_hf, dtype=torch.float32).to(device)).clone().detach().cpu().numpy().squeeze()\n",
    "basic_r2_score = results.r2_score(y_valid, basic_model(x_valid_t).clone().detach().cpu().numpy().squeeze())\n",
    "\n",
    "# R2 on hf\n",
    "adjusted_r2_score = results.r2_score(y_valid, model(x_valid_t).clone().detach().cpu().numpy().squeeze())\n",
    "\n",
    "print(f\"Basic R2 score: {basic_r2_score}\")\n",
    "print(f\"Adjusted R2 score: {adjusted_r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.measure_smoothness(basic_y_pred_hf, preprocessor.dates_high_freq, preprocessor.country_high_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.measure_smoothness(y_pred_hf, preprocessor.dates_high_freq, preprocessor.country_high_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.measure_smoothness(ultra_smooth_y_pred_hf, preprocessor.dates_high_freq, preprocessor.country_high_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare on multiple weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hf_squeezed = {\n",
    "    'date': preprocessor.dates_high_freq,\n",
    "    'country': preprocessor.country_high_freq,\n",
    "    'data': x_hf_t.clone().detach().cpu().numpy().squeeze(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_train_fn = lambda seed: train_nn(X_train, \n",
    "                                 y_train, \n",
    "                                 X_valid, \n",
    "                                 y_valid,\n",
    "                                 num_epochs=100, \n",
    "                                 learning_rate=1e-4, \n",
    "                                 weight_decay=1e-2, \n",
    "                                 verbose=False, \n",
    "                                 seed=seed)\n",
    "\n",
    "smooth_train_fn = lambda seed: train_nn(augmented_data, \n",
    "                                 y_augmented_train, \n",
    "                                 X_valid, \n",
    "                                 y_valid,\n",
    "                                 num_epochs=100, \n",
    "                                 learning_rate=1e-4, \n",
    "                                 weight_decay=1e-2, \n",
    "                                 verbose=False, \n",
    "                                 seed=seed, \n",
    "                                 custom_loss=lambda x, y: custom_loss(x, y,  [smoothness_weight_ord1, smoothness_weight_ord2, 0, 0, 0, 0]))\n",
    "\n",
    "other_losses_importance = 0.5\n",
    "ultra_smooth_train_fn = lambda seed: train_nn(augmented_data, \n",
    "                                                y_augmented_train, \n",
    "                                                X_valid, \n",
    "                                                y_valid, \n",
    "                                                num_epochs=100, \n",
    "                                                learning_rate=1e-4, \n",
    "                                                weight_decay=1e-2, \n",
    "                                                verbose=False,\n",
    "                                                current_gdp_idx=-1,\n",
    "                                                seed=seed,\n",
    "                                              custom_loss=lambda x, y: custom_loss(x, y, [smoothness_weight_ord1, smoothness_weight_ord2] + 4 * [other_losses_importance]))\n",
    "\n",
    "\n",
    "basic_bootstrapped_results = results.bootstrap_ensemble(X_valid, y_valid, basic_train_fn, n_ensembling=100, seed=SEED, device=get_device(False), other_pred_set=df_hf_squeezed)\n",
    "smooth_bootstrapped_results = results.bootstrap_ensemble(X_valid, y_valid, smooth_train_fn, n_ensembling=100, seed=SEED, device=get_device(False), other_pred_set=df_hf_squeezed)\n",
    "ultra_smooth_bootstrapped_results = results.bootstrap_ensemble(X_valid, y_valid, ultra_smooth_train_fn, n_ensembling=100, seed=SEED, device=get_device(False), other_pred_set=df_hf_squeezed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(ensemble, metric_name):\n",
    "    sns.histplot(ensemble, bins=30, kde=True)\n",
    "    plt.axvline(x=np.median(ensemble), color='darkorange', linestyle='--', label=f'Median {metric_name}', linewidth=2)\n",
    "\n",
    "    plt.xlabel(metric_name)\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Distribution of {metric_name} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plot_summary(basic_bootstrapped_results['rsquared_ensemble'], \"$R^2$ Score\")\n",
    "plt.xlim(-0.2, 1)\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_summary(smooth_bootstrapped_results['rsquared_ensemble'], \"$R^2$ Score\")\n",
    "plt.xlim(-0.2, 1)\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_summary(ultra_smooth_bootstrapped_results['rsquared_ensemble'], \"$R^2$ Score\")\n",
    "plt.xlim(-0.2, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plot_summary(basic_bootstrapped_results['other_pred_smoothness'], \"Smoothness Error\")\n",
    "plt.xlim(0.1, 0.65)\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_summary(smooth_bootstrapped_results['other_pred_smoothness'], \"Smoothness Error\")\n",
    "plt.xlim(0.1, 0.6)\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_summary(ultra_smooth_bootstrapped_results['other_pred_smoothness'], \"Smoothness Error\")\n",
    "plt.xlim(0.1, 0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "ord1_smoothness_weights = np.linspace(30, 50, 10) + [0]\n",
    "ord2_smoothness_weights = np.linspace(0, 30, 10)\n",
    "other_weights = np.linspace(0, 10, 20)\n",
    "\n",
    "def train_and_evaluate(ord1_weight, ord2_weight, other_weight):\n",
    "    model, training_loss, validation_loss, validation_r_squared, mse_loss = train_nn(\n",
    "        other_augmented_data,\n",
    "        other_y_augmented_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        num_epochs=100,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-2,\n",
    "        verbose=False,\n",
    "        custom_loss=lambda x, y: custom_loss(x, y, [ord1_weight, ord2_weight] + 4 * [other_weight]),\n",
    "        current_gdp_idx=-1,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    y_pred_hf = model(torch.tensor(x_hf, dtype=torch.float32).to(device)).clone().detach().cpu().numpy().squeeze()\n",
    "    smoothness_loss = results.measure_smoothness(y_pred_hf, preprocessor.dates_high_freq, preprocessor.country_high_freq)\n",
    "\n",
    "    return (ord1_weight, ord2_weight, other_weight, training_loss, validation_loss, validation_r_squared, smoothness_loss)\n",
    "\n",
    "# Run in parallel\n",
    "param_combinations = [(ord1, ord2, other_weight) for ord1 in ord1_smoothness_weights for ord2 in ord2_smoothness_weights for other_weight in other_weights]\n",
    "\n",
    "parallel_results = Parallel(n_jobs=-1, backend='loky')(\n",
    "    delayed(train_and_evaluate)(ord1, ord2, other_weight) for ord1, ord2, other_weight in tqdm(param_combinations)\n",
    ")\n",
    "\n",
    "# Unpack results\n",
    "ord1_weight, ord2_weight, other_weight, training_loss, validation_loss, validation_r_squared, smoothness_loss = zip(*parallel_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parallel results\n",
    "parallel_results_df = pd.DataFrame({\n",
    "    'ord1_weight': ord1_weight,\n",
    "    'ord2_weight': ord2_weight,\n",
    "    'other_weight': other_weight,\n",
    "    'training_loss': training_loss,\n",
    "    'validation_loss': validation_loss,\n",
    "    'validation_r_squared': validation_r_squared,\n",
    "    'smoothness_loss': smoothness_loss\n",
    "})\n",
    "\n",
    "parallel_results_df.to_pickle('paper_data/parallel_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_top_filtered_percent(data, x_vals, y_vals, title, xlabel, ylabel, cmap='viridis', threshold=5):\n",
    "    # Compute the 90th percentile threshold\n",
    "    valid_data = data[np.isfinite(data)]\n",
    "    if len(valid_data) == 0:\n",
    "        print(\"No valid data to plot.\")\n",
    "        return\n",
    "    #threshold = np.percentile(valid_data, 90)\n",
    "    \n",
    "    # Mask values below the threshold\n",
    "    data_filtered = np.where((data >= -threshold) & (data <= threshold), data, np.nan)\n",
    "    \n",
    "    im = plt.imshow(data_filtered, origin='lower', \n",
    "                    extent=[min(x_vals), max(x_vals), min(y_vals), max(y_vals)], \n",
    "                    aspect='auto', cmap=cmap) #, norm=norm)\n",
    "    \n",
    "    # Hilight the maximum value\n",
    "    max_val = np.nanmax(data_filtered)\n",
    "    max_idx = np.where(data_filtered == max_val)\n",
    "    # Centered in the middle of the square\n",
    "    # plt.scatter(x_vals[max_idx[1]],\n",
    "    #             y_vals[max_idx[0]],\n",
    "    #             color='red', marker='x', s=30)\n",
    "\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label('Metric Value')\n",
    "    plt.title(title + f' (Between -{threshold} and {threshold})')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord1_weight, ord2_weight, other_weight, training_loss, validation_loss, validation_r_squared, smoothness_loss = zip(*parallel_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = len(ord1_smoothness_weights)\n",
    "N2 = len(ord2_smoothness_weights)\n",
    "N3 = len(other_weights)\n",
    "\n",
    "# Create mapping from (ord1_weight, ord2_weight) to grid indices\n",
    "ord1_to_idx = {w: i for i, w in enumerate(ord1_smoothness_weights)}\n",
    "ord2_to_idx = {w: i for i, w in enumerate(ord2_smoothness_weights)}\n",
    "ord_other_idx = {w: i for i, w in enumerate(other_weights)}\n",
    "\n",
    "# Initialize empty arrays for final metrics\n",
    "final_train_loss = np.zeros((N1, N2, N3))\n",
    "final_val_loss = np.zeros((N1, N2, N3))\n",
    "final_val_r2 = np.zeros((N1, N2, N3))\n",
    "final_smoothness_mean = np.zeros((N1, N2, N3))\n",
    "final_validation_mse_loss = np.zeros((N1, N2, N3))  # example if needed\n",
    "final_training_mse_loss = np.zeros((N1, N2, N3))  # example if needed\n",
    "\n",
    "for ord1, ord2, other_weight, val_r2, sm_loss in zip(ord1_weight, ord2_weight, other_weight, validation_r_squared, smoothness_loss):\n",
    "    i = ord1_to_idx[ord1]\n",
    "    j = ord2_to_idx[ord2]\n",
    "    z = ord_other_idx[other_weight]\n",
    "\n",
    "    # final_train_loss[i, j, z] = tr_loss[-1] if len(tr_loss) > 0 else np.nan\n",
    "    # final_val_loss[i, j, z] = val_loss[-1] if len(val_loss) > 0 else np.nan\n",
    "    final_val_r2[i, j, z] = val_r2[-1] if len(val_r2) > 0 else np.nan\n",
    "    \n",
    "    final_smoothness_mean[i, j, z] = sm_loss[1] if len(sm_loss) > 0 else np.nan\n",
    "    # final_validation_mse_loss[i, j] = mse_loss[-1][1] if len(mse_loss) > 0 else np.nan\n",
    "    # final_training_mse_loss[i, j] = mse_loss[-1][0] if len(mse_loss) > 0 else np.nan\n",
    "\n",
    "# Plot R^2 over the smoothness strength scatter plot for all values\n",
    "plt.figure(figsize=(6, 4), dpi=150)\n",
    "plt.scatter(final_smoothness_mean.flatten(), final_val_r2.flatten(), c=(final_val_r2-final_smoothness_mean).flatten(), cmap='viridis', marker='x')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Smoothness Loss')\n",
    "plt.ylabel('Final Validation $R^2$')\n",
    "plt.title('Final Validation $R^2$ vs. Smoothness Loss for Different Ord1 and Ord2 Weights')\n",
    "plt.show()\n",
    "np.nanmax(final_val_r2-final_smoothness_mean), np.unravel_index(np.nanargmax(final_val_r2-final_smoothness_mean), final_val_r2.shape)\n",
    "idxs = np.unravel_index(np.nanargmax(final_val_r2-final_smoothness_mean), final_val_r2.shape)\n",
    "best_ord1 = ord1_smoothness_weights[idxs[0]]\n",
    "best_ord2 = ord2_smoothness_weights[idxs[1]]\n",
    "best_other = other_weights[idxs[2]]\n",
    "\n",
    "np.save('paper_data/smoothing_objective_function_all_metrics_6_batchsize.npy', (ord1_weight, ord2_weight, other_weight, final_val_r2, final_smoothness_mean))\n",
    "\n",
    "best_ord1, best_ord2, best_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(ord1_weight, ord2_weight, other_weight):\n",
    "    model, training_loss, validation_loss, validation_r_squared, mse_loss = train_nn(\n",
    "        augmented_data,\n",
    "        y_augmented_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        num_epochs=100,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-2,\n",
    "        verbose=False,\n",
    "        custom_loss=lambda x, y: custom_loss(x, y, [ord1_weight, ord2_weight] + 4 * [other_weight]),\n",
    "        current_gdp_idx=-1,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    y_pred_hf = model(torch.tensor(x_hf, dtype=torch.float32).to(device)).clone().detach().cpu().numpy().squeeze()\n",
    "    smooth_loss = results.measure_smoothness(y_pred_hf, preprocessor.dates_high_freq, preprocessor.country_high_freq)\n",
    "\n",
    "    return (ord1_weight, ord2_weight, other_weight, training_loss, validation_loss, validation_r_squared, smooth_loss)\n",
    "\n",
    "# Run in parallel\n",
    "param_combinations = [(ord1, ord2, other_weight) for ord1 in ord1_smoothness_weights for ord2 in ord2_smoothness_weights for other_weight in other_weights]\n",
    "\n",
    "parallel_results = Parallel(n_jobs=-1, backend='loky')(\n",
    "    delayed(train_and_evaluate)(ord1, ord2, other_weight) for ord1, ord2, other_weight in tqdm(param_combinations)\n",
    ")\n",
    "\n",
    "# Unpack results\n",
    "ord1_weight, ord2_weight, other_weight, training_loss, validation_loss, validation_r_squared, smooth_loss = zip(*parallel_results)\n",
    "\n",
    "# Save parallel results\n",
    "parallel_results_4_batchsize_df = pd.DataFrame({\n",
    "    'ord1_weight': ord1_weight,\n",
    "    'ord2_weight': ord2_weight,\n",
    "    'other_weight': other_weight,\n",
    "    'training_loss': training_loss,\n",
    "    'validation_loss': validation_loss,\n",
    "    'validation_r_squared': validation_r_squared,\n",
    "    'smooth_loss': smooth_loss\n",
    "})\n",
    "\n",
    "parallel_results_4_batchsize_df.to_pickle('paper_data/smoothing_objective_function_all_metrics_4_batchsize.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
