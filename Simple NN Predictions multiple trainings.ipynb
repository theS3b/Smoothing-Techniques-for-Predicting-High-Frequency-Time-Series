{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.load_data import load_data, load_gt_data\n",
    "from utils.preprocessing import Preprocessing\n",
    "from models.MLP import MLP\n",
    "from models.LinearModels import OLS, RidgeRegression\n",
    "from models.KalmanFilterMLP import KalmanFilterMLP\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "TRENDS_FOLDER = 'data/google_trends/'\n",
    "GDP_FOLDER = 'data/gdp/'\n",
    "DATA_PREFIX = 'trends_data_by_topic_'\n",
    "\n",
    "EPS = 1e-15\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ALL_GT_DATA = False # If set to True, the model will be trained on all available data and predictions will be made for all available GT data\n",
    "\n",
    "TRAIN_PROPORTION = 0.94 if not TEST_ALL_GT_DATA else 1\n",
    "PAST_GDPS = [] if not TEST_ALL_GT_DATA else None # e.g. range(1, 3) or [1, 2]\n",
    "MODE = \"pct\" # None | \"pct\" | \"diff\"\n",
    "PERIOD = 4  # Year to year prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_gdps, all_gts = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = []\n",
    "search_terms = [col for col in all_gts.columns if col.endswith('_average')]\n",
    "data.sort_values(by=['date', 'country'], inplace=True)\n",
    "data['date'] = pd.to_datetime(data['date']).dt.strftime('%Y-%m-%d')\n",
    "data_copied = data.copy()\n",
    "data_copied['date'] = pd.to_datetime(data_copied['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 1. Direct Google Trends Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_preprocessed.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 2. Google Trends Data with Lagged Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_PAST_QUARTERS = 8\n",
    "\n",
    "processed_gts = all_gts.copy()\n",
    "processed_gts['date'] = pd.to_datetime(processed_gts['date'])\n",
    "\n",
    "# Take the log of the search terms\n",
    "processed_gts[search_terms] = np.log(processed_gts[search_terms] + 1)\n",
    "\n",
    "# Add the difference of log search terms current - current-1Q, current - current-2Q, etc.\n",
    "for nb_quarters in range(1, NB_PAST_QUARTERS + 1):\n",
    "    diff = (processed_gts[search_terms] - processed_gts.groupby(\"country\")[search_terms].diff(3 * nb_quarters)).add_prefix(f'q{nb_quarters}-')\n",
    "    processed_gts = pd.concat([processed_gts, diff], axis=1)\n",
    "\n",
    "# Drop the current values as they are already in data\n",
    "processed_gts.drop(columns=search_terms, inplace=True)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "processed_gts = processed_gts.dropna()\n",
    "data_merged = data_copied.merge(processed_gts, left_on=[\"country\", \"date\"], right_on=[\"country\", \"date\"], how=\"left\")\n",
    "data_merged['date'] = pd.to_datetime(data_merged['date']).dt.strftime('%Y-%m-%d')\n",
    "data_merged.dropna(inplace=True)\n",
    "data_preprocessed.append(data_merged)\n",
    "\n",
    "print(f\"Data merged shape: {data_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 3. Google Trends Data with average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_PAST_QUARTERS = 8\n",
    "\n",
    "processed_gts = all_gts.copy()\n",
    "processed_gts['date'] = pd.to_datetime(processed_gts['date'])\n",
    "\n",
    "# Add gts from 1 year ago (ok because we sort by date then by country)\n",
    "processed_gts = pd.concat([processed_gts[['country', 'date']], processed_gts[search_terms].shift(12).add_prefix('1yo_old_')], axis=1)\n",
    "\n",
    "# Add the average of the search terms over the past 4 quarters\n",
    "rolling_result = all_gts.groupby('country')[search_terms + ['date']].rolling(4).mean(numeric_only=True).reset_index()\n",
    "rolling_result['date'] = all_gts.loc[rolling_result['level_1'], 'date'].values\n",
    "rolling_result = rolling_result.drop(columns='level_1')\n",
    "rolling_result.sort_values(by=['country', 'date'], inplace=True)\n",
    "processed_gts = pd.concat([processed_gts, rolling_result[search_terms].add_prefix('1yo_avg_')], axis=1)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "processed_gts = processed_gts.dropna()\n",
    "data_merged = data_copied.merge(processed_gts, left_on=[\"country\", \"date\"], right_on=[\"country\", \"date\"], how=\"left\")\n",
    "data_merged['date'] = pd.to_datetime(data_merged['date']).dt.strftime('%Y-%m-%d')\n",
    "data_merged.dropna(inplace=True)\n",
    "data_preprocessed.append(data_merged)\n",
    "\n",
    "print(f\"Data merged shape: {data_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_date = data_copied['date'].quantile(TRAIN_PROPORTION)\n",
    "splitting_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preprocessed = []\n",
    "for dataset in data_preprocessed:\n",
    "    preprocessor = Preprocessing(data=dataset, epsilon=EPS, mode=MODE, past_GDP_lags=PAST_GDPS, diff_period=PERIOD, all_GDPs=all_gdps, all_GTs=all_gts)\n",
    "    X_train, y_train, X_valid, y_valid = preprocessor.preprocess_data(train_pct=TRAIN_PROPORTION, splitting_date=splitting_date)\n",
    "    \n",
    "    all_preprocessed.append((X_train.values, y_train.values, X_valid.values, y_valid.values, preprocessor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start with a simple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "    \n",
    "# Function to set random seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU (if available)\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch for all GPUs (if multiple GPUs are used)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Avoids non-deterministic optimizations\n",
    "    \n",
    "def train_nn(x_train, y_train, x_valid, y_valid, num_epochs=1000, learning_rate=1e-3, weight_decay=1e-3, seed=42, verbose=True):\n",
    "    set_seed(seed)\n",
    "\n",
    "    num_features = x_train.shape[1]\n",
    "    model = NeuralNetwork(num_features=num_features).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    x_train_t = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "    x_valid_t = torch.tensor(x_valid, dtype=torch.float32).to(device)\n",
    "    y_valid_t = torch.tensor(y_valid, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "\n",
    "    training_range = tqdm(range(num_epochs)) if verbose else range(num_epochs)\n",
    "    \n",
    "    for t in training_range:\n",
    "        model.train()\n",
    "        y_pred = model(x_train_t)\n",
    "        loss_train = loss_fn(y_pred, y_train_t)\n",
    "\n",
    "        model.eval()\n",
    "        loss_valid = loss_fn(model(x_valid_t), y_valid_t)\n",
    "        model.train()\n",
    "\n",
    "        training_loss.append(loss_train.item())\n",
    "        validation_loss.append(loss_valid.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    y_pred = model(x_valid_t)\n",
    "    loss = loss_fn(y_pred, y_valid_t)\n",
    "    print(f\"Validation loss: {loss.item()}\")\n",
    "    \n",
    "    return model, training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []\n",
    "seeds = range(123, 173)\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    for x_train, y_train, x_valid, y_valid, preprocessor in all_preprocessed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        model, training_loss, validation_loss = train_nn(x_train, y_train, x_valid, y_valid, num_epochs=25, learning_rate=1e-3, weight_decay=1e-4, seed=seed, verbose=False)\n",
    "\n",
    "        # Get the predictions\n",
    "        x_valid_for_pred = torch.tensor(x_valid, dtype=torch.float32).to(device)\n",
    "        x_train_for_pred = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "        y_pred = model(x_valid_for_pred).clone().detach().cpu().numpy().squeeze()\n",
    "        y_pred_train = model(x_train_for_pred).clone().detach().cpu().numpy().squeeze()\n",
    "\n",
    "        trained_models.append((model, training_loss, validation_loss, y_pred, y_pred_train, preprocessor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "for i, (model, training_loss, validation_loss, y_pred, y_pred_train, preprocessor) in enumerate(trained_models):\n",
    "    plt.plot(training_loss, label=f\"Training loss {i}\")\n",
    "    plt.plot(validation_loss, label=f\"Validation loss {i}\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.xlim(0, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_shapes = [y_pred_train.shape[0] for _, _, _, _, y_pred_train, _ in trained_models]\n",
    "min_elems_y_pred_train = np.min(y_pred_train_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all predictions for validation and training sets\n",
    "y_pred_valid_all = np.array([y_pred for _, _, _, y_pred, _, _ in trained_models])\n",
    "y_pred_train_all = np.array([y_pred_train[-min_elems_y_pred_train:] for _, _, _, _, y_pred_train, _ in trained_models])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean predictions\n",
    "y_pred_valid_mean = np.mean(y_pred_valid_all, axis=0)\n",
    "y_pred_train_mean = np.mean(y_pred_train_all, axis=0)\n",
    "\n",
    "# Compute standard deviations\n",
    "y_pred_valid_std = np.std(y_pred_valid_all, axis=0)\n",
    "y_pred_train_std = np.std(y_pred_train_all, axis=0)\n",
    "\n",
    "# Compute 95% prediction intervals\n",
    "y_pred_valid_lower = np.percentile(y_pred_valid_all, 2.5, axis=0)\n",
    "y_pred_valid_upper = np.percentile(y_pred_valid_all, 97.5, axis=0)\n",
    "\n",
    "y_pred_train_lower = np.percentile(y_pred_train_all, 2.5, axis=0)\n",
    "y_pred_train_upper = np.percentile(y_pred_train_all, 97.5, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "mean_squared_error(y_valid, y_pred_valid_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_valid, y_pred_valid_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation set\n",
    "y_pred_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_valid,\n",
    "    'country': preprocessor.country_valid,\n",
    "    'y_pred_mean': y_pred_valid_mean,\n",
    "    'y_pred_lower': y_pred_valid_lower,\n",
    "    'y_pred_upper': y_pred_valid_upper,\n",
    "    'y_true': y_valid\n",
    "})\n",
    "\n",
    "# For training set\n",
    "y_pred_train_country = pd.DataFrame({\n",
    "    'date': preprocessor.dates_train[-min_elems_y_pred_train:],\n",
    "    'country': preprocessor.country_train[-min_elems_y_pred_train:],\n",
    "    'y_pred_mean': y_pred_train_mean,\n",
    "    'y_pred_lower': y_pred_train_lower,\n",
    "    'y_pred_upper': y_pred_train_upper,\n",
    "    'y_true': y_train[-min_elems_y_pred_train:]\n",
    "})\n",
    "\n",
    "# Combine the predictions\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together the train and the validation set\n",
    "predictions = pd.concat([y_pred_train_country, y_pred_country])\n",
    "\n",
    "def plot_by_country(selected_country):\n",
    "    filtered_data = predictions[predictions[\"country\"] == selected_country]\n",
    "    filtered_data = filtered_data.sort_values(by='date')\n",
    "    cutoff_date = splitting_date\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true values\n",
    "    plt.plot(filtered_data['date'], filtered_data['y_true'], label='True Values', color='blue', marker='.')\n",
    "    \n",
    "    # Plot the predicted mean\n",
    "    plt.plot(filtered_data['date'], filtered_data['y_pred_mean'], label='Predicted Mean', color='orange', marker='x')\n",
    "\n",
    "    \n",
    "    # Add the prediction interval\n",
    "    plt.fill_between(\n",
    "        filtered_data['date'],\n",
    "        filtered_data['y_pred_lower'],\n",
    "        filtered_data['y_pred_upper'],\n",
    "        color='orange', alpha=0.2, label='95% Prediction Interval'\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Prediction vs True Values for {selected_country}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.axvline(x=cutoff_date, color='red', linestyle='--', label=f'Validation Start ({TRAIN_PROPORTION}%)')\n",
    "    plt.legend(title=\"Legend\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a dropdown widget for selecting the country\n",
    "countries = predictions[\"country\"].unique()\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=countries,\n",
    "    value=countries[0],\n",
    "    description='Country:'\n",
    ")\n",
    "\n",
    "# Use the interact function to link the dropdown with the plot function\n",
    "interact(plot_by_country, selected_country=dropdown)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (model, training_loss, validation_loss, y_pred, y_pred_train, preprocessor) in enumerate(trained_models):\n",
    "    plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl-master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
